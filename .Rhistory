# subsetting small sampling sizes to see better what happen there
minigamma<-gammaData[gammaData$samplingSize<33,]
# making factors for the ggplot2 pltos
factor(gammaData$Pop)->gammaData$pops
factor(minigamma$Pop)->minigamma$minipops
factor(minigamma$samplingSize)->minigamma$samplingSizeF
# plotting with ggplot, need to fix legends
#http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/
# Fig X2a
ggplot(gammaData, aes(x = samplingSize,y=Max.Gamma.value,fill=pops,color=pops)) + geom_point()+stat_smooth(method="loess")+geom_abline()+scale_fill_discrete(name="Populations",breaks=c("Pop1","Pop2","Pop3","Pop4"),labels=c("Theta=2g", "Theta=2ng","Theta=10g", "Theta=10ng"))
# Fig X2b
# thinking how to plot the subset of samples sizes where gamma works a little bit better....
ggplot(minigamma, aes(x = samplingSizeF,y=Max.Gamma.value,fill=minipops,color=minipops)) + geom_boxplot(aes(fill=minipops))+ylim(0,20)
ggplot(minigamma, aes(x = samplingSizeF,y=Max.Gamma.value,fill=minipops,color=minipops)) + geom_boxplot(aes(fill=minipops))+ylim(0,20)+geom_abline()
data<-read.csv("numberSegSites.csv",header=T)
# we need to define a maximum n, John identified that above 70 this behaves funny, actually for 60 too, so I put 50
maxn=50
maxk<-max(data$segSites)
data<-read.csv("numberSegSites.csv",header=T)
str(data)
Q = 2 #theta
# creating empty list to put the results
theta2<-list()
# looping from 1 to maxk and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta2[[name]]
}
}
maxk
maxk<-max(data$n.seg.sites,na.rm=T)
maxk
theta2<-list()
# looping from 1 to maxk and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta2[[name]]
}
}
theta2[[1]]
theta.2 <-t(data.frame(do.call(rbind, theta2)))
as.data.frame(theta.2)->data2
data2$Theta<-"two"
data2$n<-seq(1,50,1)
row.names(data10)<-NULL
View(data2)
str(data2)
length(theta2)
maxk
data2$n<-seq(1,50,1)
str(data2)
tail(data2)
class(data2)
# -----theta=2-----------
Q = 2 #theta
# creating empty list to put the results
theta2<-list()
# looping from 1 to maxk and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta2[[name]]
}
}
# The result of the loop is a list "theta2", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.
# put data in a dataframe to handle it more easy
theta.2 <-t(data.frame(do.call(rbind, theta2)))
as.data.frame(theta.2)->data2
data2$Theta<-"two"
data2$n<-seq(1,50,1)
row.names(data2)<-NULL
# -----theta=10-----------
Q = 10 #theta
# creating empty list to put the results
theta10<-list()
# looping in segdata and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta10[[name]]
}
}
# The result of the loop is a list "theta10", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.
# put data in a dataframe to handle it more easy
theta10 <-t(data.frame(do.call(rbind, theta10)))
as.data.frame(theta.10)->data10
data10$Theta<-"ten"
data10$n<-seq(1,50,1)
row.names(data10)<-NULL
# -----theta=20-----------
Q = 20 #theta
# creating empty list to put the results
theta10<-list()
# looping in segdata and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta20[[name]]
}
}
# The result of the loop is a list "theta20", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.
# put data in a dataframe to handle it more easy
theta20 <-t(data.frame(do.call(rbind, theta20)))
as.data.frame(theta.20)->data20
data20$Theta<-"twenty"
data20$n<-seq(1,50,1)
row.names(data20)<-NULL
data<-read.csv("numberSegSites.csv",header=T)
# we need to define a maximum n, John identified that above 70 this behaves funny, actually for 60 too, so I put 50
maxn=50
maxk<-max(data$n.seg.sites,na.rm=T)
# -----theta=2-----------
Q = 2 #theta
# creating empty list to put the results
theta2<-list()
# looping from 1 to maxk and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta2[[name]]
}
}
# The result of the loop is a list "theta2", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.
# put data in a dataframe to handle it more easy
theta.2 <-t(data.frame(do.call(rbind, theta2)))
as.data.frame(theta.2)->data2
data2$Theta<-"two"
data2$n<-seq(1,50,1)
row.names(data2)<-NULL
# -----theta=10-----------
Q = 10 #theta
# creating empty list to put the results
theta10<-list()
# looping in segdata and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta10[[name]]
}
}
# The result of the loop is a list "theta10", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.
# put data in a dataframe to handle it more easy
theta.10 <-t(data.frame(do.call(rbind, theta10)))
as.data.frame(theta.10)->data10
data10$Theta<-"ten"
data10$n<-seq(1,50,1)
row.names(data10)<-NULL
# -----theta=20-----------
Q = 20 #theta
# creating empty list to put the results
theta10<-list()
# looping in segdata and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta20[[name]]
}
}
# The result of the loop is a list "theta20", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.
# put data in a dataframe to handle it more easy
theta.20 <-t(data.frame(do.call(rbind, theta20)))
as.data.frame(theta.20)->data20
data20$Theta<-"twenty"
data20$n<-seq(1,50,1)
row.names(data20)<-NULL
# join dataframes and save file
rbind(data2,data10,data20)->WakeleyData
# -----theta=2-----------
Q = 2 #theta
# creating empty list to put the results
theta2<-list()
# looping from 1 to maxk and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta2[[name]]
}
}
# The result of the loop is a list "theta2", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.
# put data in a dataframe to handle it more easy
theta.2 <-t(data.frame(do.call(rbind, theta2)))
as.data.frame(theta.2)->data2
data2$Theta<-"two"
data2$n<-seq(1,50,1)
row.names(data2)<-NULL
# -----theta=10-----------
Q = 10 #theta
# creating empty list to put the results
theta10<-list()
# looping in segdata and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta10[[name]]
}
}
# The result of the loop is a list "theta10", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.
# put data in a dataframe to handle it more easy
theta.10 <-t(data.frame(do.call(rbind, theta10)))
as.data.frame(theta.10)->data10
data10$Theta<-"ten"
data10$n<-seq(1,50,1)
row.names(data10)<-NULL
# -----theta=20-----------
Q = 20 #theta
# creating empty list to put the results
theta20<-list()
# looping in segdata and running Wakeley formula
for (j in 1:maxk){
name<-paste("seg",j,sep="")
obsvdk<-j
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b
for (n in 2:maxn) {
PSK=0
for (i in 2:n) {
PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
}
PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
obsvd<-PSKv[obsvdk,]
obsvd->theta20[[name]]
}
}
# The result of the loop is a list "theta20", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.
# put data in a dataframe to handle it more easy
theta.20 <-t(data.frame(do.call(rbind, theta20)))
as.data.frame(theta.20)->data20
data20$Theta<-"twenty"
data20$n<-seq(1,50,1)
row.names(data20)<-NULL
# join dataframes and save file
rbind(data2,data10,data20)->WakeleyData
write.csv(WakeleyDa,ta,"WakeleyData.csv")
#hist3D(x=seq(0,1,length.out=nrow(PSKv)),y=seq(0,1,length.out=ncol(PSKv)),PSKv,col=NULL,border="black",theta=2,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35,main=paste((expression(theta))," = ",Q))
#  print(obsvd) #now would be easy to print 95% HPD of obsvd, so for given theta and k this is the distribution of n
# plot(obsvd,xlab="n",ylab="P(n|k)",ylim=c(0,0.15),main=paste("for K =",obsvdk, "and theta =",Q,"in black; for hapdiv in red"))
```
The result of the loop is two list, one for each theta, "ndistT2" and "ndistT10", with each component being one the observed segregating site values and quantiles, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.
Now in our "QuBar_current" we can plot for each sampling size the expected value according Wakeley, and mark the abline with the observed value. And then do that for both theta.
```{r plotting Wakeley data}
library(lattice)
#load Wakeley data and observed segregating sites
data<-read.csv("hapsegData.csv",header=T)
wakdata<-read.csv("WakeleyData.csv",header=T)
miniwak<-wakdata[which(wakdata$n==c(2,4,8,16,32),]
# Summarize info on observed segregating sites for each population
subset(data,Populations=="Pop1")->pop1
subset(data,Populations=="Pop2")->pop2
subset(data,Populations=="Pop3")->pop3
subset(data,Populations=="Pop4")->pop4
# plot histogram to see distribution of segSites
histogram(~segSites|factor(sampleSize),data=pop1, main="Population 1-t2g")
histogram(~segSites|factor(sampleSize),data=pop2,main="Population 2-t2ng")
histogram(~segSites|factor(sampleSize),data=pop3,main="Population 3-t10g")
histogram(~segSites|factor(sampleSize),data=pop4,main="Population 4-t10ng")
# get summary information to see median and quantiles
aggregate(pop1$segSites,by=list(pop1$sampleSize),FUN=summary)
aggregate(pop2$segSites,by=list(pop2$sampleSize),FUN=summary)
aggregate(pop3$segSites,by=list(pop3$sampleSize),FUN=summary)
aggregate(pop4$segSites,by=list(pop4$sampleSize),FUN=summary)
# Plotting the data..thinking how is the best way...using the 1 and 3th quantiles plus the median
plots(ndistT10[[1]])
points(ndistT10[[2]],col="green")
abline(v=2)
```
----------------until here revised------------------------------------
# Using the sampling theory of selectively neutral alleles
For what I read, this uses the number of haplotypes, theta and the sampling n. I think we can use this to backcalculate n.
```{r using sampling theory Ewens 1972}
# load data with observed haplotypesin the samples
data<-read.csv("hapsegData.csv",header=T)
data$segSites<-NULL
data$X<-NULL
factor(data$sampleSize)->data$sampleSize
databp<-data[with(data,order(Populations,sampleSize,Haplotypes)),]
row.names(databp)<-NULL
# subsetting populations
pop1<-databp[which(databp$Populations=='Pop1'),]
pop2<-databp[which(databp$Populations=='Pop2'),]
pop3<-databp[which(databp$Populations=='Pop3'),]
pop4<-databp[which(databp$Populations=='Pop4'),]
# calculate all the different k haplotypes given different values of n
theta=2 #define theta
maxn=128 #define max n
# this loop fills a vector with the expected k for a given theta an differents n
meank<-rep(NA,maxn)
for(n in 2:maxn){
n-1->endpoint
res<-rep(NA,endpoint)
for (i in 1:endpoint){
theta/(theta+i)->res[i]
}
1+sum(res)->meank[n]
}
# this loop fills a vector with the variance of k for a given theta an differents n
vark<-rep(NA,maxn)
for(n in 2:maxn){
n-1->endpoint
res<-rep(NA,endpoint)
for (i in 1:endpoint){
(theta^2)/((theta+i)^2)->res[i]
}
meank[n]-sum(res)->vark[n]
}
# now we put together a dataframe with the n,k,and var(k), run it twice changing theta to make dataframe
meank->meank2; vark->vark2
meank->meank10; vark->vark10
dataSampling<-as.data.frame(cbind(c(1:maxn),meank2,vark2,meank10,vark10))
names(dataSampling)<-c("n","expHap.t2","varHap.t2","expHap.t10","varHap.t10")
# plot variance
with(dataSampling,plot(n,varHap.t10,col="red"))
with(dataSampling,points(n,varHap.t2,col="blue"))
legend("topleft", inset=.05, c("theta=2","theta=10"), fill=c("blue","red"), horiz=TRUE)
# save data file
write.csv(dataSampling,"dataSamplingTheory.csv")
```
# Old code just in case
```{r OLD gamma distribution code,eval=F}
# load libraries
library(lattice)
library(ggplot2)
library(vioplot)
# Number of samples, from a log2 distribution, it is the same for all populations
numsamp<-c(2,4,8,16,32,64,128)
# Haplotype diversity for each of the four simulated populations of 1000 individuals
hapdiv<-c(0.5504204, 0.3377538, 0.8915536, 0.9258679)
# Number of haplotypes for each replicates are in a list within a list
# pop1haps (list of 7 sampling sizes, that includes the list with 100 replicates)
# pop2haps
# pop3haps
# pop4haps
# We can make this more efficient, but for now, let's keep a loop for each population:
# set dataframe to fill with the predicted and observed values,for each population, we need
Population<-rep(NA,700)
Max.Pred.value<-rep(NA,700)
data.frame(Population,Max.Pred.value)->toFill
toFill->toFill2
toFill->toFill3
toFill->toFill4
rbind(data2,data10,data20)->WakeleyData
head(WakeleyData)
all<-melt(WakeleyData,id=c("Theta","n"))
head(all)
tail(all)
class(all)
str(all)
levels(all$variable)<-seq(1:135)
str(all)
write.csv(WakeleyData,"WakeleyData.csv")
write.csv(all,"WakeleyData.csv")
names(all)<-c("Theta","n","segSites","prob")
head(all)
as.numeric(all$variable)->all$variable
class(all$variable)
str(WakeleyData)
all<-melt(WakeleyData,id=c("Theta","n"))
str(all)
as.numeric(levels(all$variable))[all$variable]
rbind(data2,data10,data20)->WakeleyData
rbind(data2,data10,data20)->WakeleyData
# reorder the data
all<-melt(WakeleyData,id=c("Theta","n"))
names(all)<-c("Theta","n","segSites","prob")
# save
write.csv(all,"WakeleyData.csv")
all<-melt(WakeleyData,id=c("Theta","n"))
levels(all$variable)<-seq(1:135)
names(all)<-c("Theta","n","segSites","prob")
# save
write.csv(all,"WakeleyData.csv")
write.csv(all,"WakeleyData.csv")
data<-read.csv("numberSegSites.csv",header=T)
wakdata<-read.csv("WakeleyData.csv",header=T)
miniwak<-wakdata[which(wakdata$n==c(2,4,8,16,32),]
miniwak<-wakdata[which(wakdata$n==c(2,4,8,16,32)]
miniwak<-wakdata[which(wakdata$n<33),]
