---
title: "Attempts at Quantitative Metazoan Metabarcoding are Difficult"
author: "John Wares and Paula Pappalardo"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: pdf_document
bibliography: QuBar_References_Sep2015.bib
---
```{r set working directory,echo=FALSE}
# JOHN, I think we can use this trick to avoid the issue with the working directory, after you set the working directory with "To Source File Location" (if that is the same in macs)
mydir <- getwd()
setwd(mydir)
```
# Abstract (200 words maximum)

__Key words:__(3 to ten)

# Introduction

We start with what may seem like a trivial question: assume that you have been told that a series of fair coin flips resulted in 60% 'heads', 40% 'tails'. This is the only information given, but you already have made a judgment about how many coin flips occurred, and perhaps have generated a probability distribution in your head where the highest likelihood is for 5 or 10, rather than 50 or 100, events. This is taking advantage of what we know about the probability mass function of a binomial distribution, where the observed number of 'successes' in a series is related to the probability of success (here, presumably 50%) and the number of trials.  A large error from our expectations is what suggests the low sample size from that distribution.

Here, we consider whether the same principle could be used for improving the efficiency of exploring the presence, distribution, and abundance of genetic biodiversity. Documenting the distribution and abundance of biodiversity - in many habitats, at multiple scales - is perhaps more important now than ever as scientists evaluate how populations are responding to environmental change. Though technological advances have rapidly improved some elements of this [@Nagendra01; @Bourlat13], there are still glaring deficiencies in our ability to efficiently catalog diversity, even in small domains or limited taxonomic surveys.

The most apparent advances have been in surveys of microbial and viral diversity. Next-generation sequencing (NGS) has permitted the now-commonplace exploration of fungal, bacterial, and viral diversity by generating $10^5$ - $10^7$ sequence reads per sample and using barcoding approaches (match of sequence to known taxonomic samples for that genomic region) to identify the taxa present and their relative abundance. While there is no doubt that this has transformed our understanding of functional ecosystem processes and microbial ecology at this scale [@Nguyen14; @Turn09; @Desnues08], there are definite limitations. For example, some taxa (e.g. Archaea) may not be as readily amplified using the same ribosomal 16S "bacteria" primers, and variation in amplification efficiency certainly exists within the Eubacteria [@Acinas05]. Additionally, it is known that some bacterial genomes harbor more than one copy of this canonical locus [@Kembel12], thus muddling the relationship between read frequency and taxon frequency in a community.

The same problems exist - and are exacerbated - when studying multicellular diversity. On top of the problems of potential contamination, detecting rare taxa and/or handling singleton evidence for rare taxa, there is potentially a large variance in individual sizes of organisms. This, along with amplification variation given mismatches in the primer region, means that the relative read abundance in a NGS data set will often wildly vary (by multiple orders of magnitude) from the abundance of actual tissue in the data set [@Nguyen14; @Pinol14; @Bohmann14]. Researchers tend to address this by analyzing data for simple incidence as well as relative read abundance, to identify patterns robust to either removal of information or inaccurate information [@Nguyen14].

If, however, the goal is to understand the actual relative abundance of individuals of different species in a sample - with these species harboring variation at 'barcode' loci, and often being highly divergent from one another - our question is whether there is complementary information that can be extracted from these data that does not rely on the abundance of reads that are assigned to a taxon, but relies on our understanding of diversity within populations and how that can be measured. 

The summary statistics for DNA sequence diversity are well established and generally recognize the population mutation rate $\theta$ at a given locus; as a population increases in size (N), or as the mutation rate $\mu$ at that locus increases, more polymorphisms and more diversity will be found ($\theta$ is proportional to N*x*$\mu$). There are limitations to this approach based on Kimura's neutral theory, as various forms of genomic selection will limit the direct relationship between population size and population diversity [e.g., @Bazin06; @Wares10; @Corbett15]. Nevertheless, these summary statistics - including Watterson's $\theta$, a sample-normalized estimator that uses the number of segregating sites *S* in a sample - may provide information necessary to generate *some* inference of relative abundance from NGS data. This information also has its limits: nucleotide diversity ($\pi$) requires information on polymorphic site frequencies that will be biased by differential amplification across individuals, as well as relatively uninformative - or diminishing - returns as the number of sampled individuals increases [@Wakeley08]. Haplotype diversity (*H*) is likely sufficient to set a minimum boundary on the number of individuals sampled, and *H* along with *S* may provide enough information to generate a probabilistic distribution associated with larger numbers of individuals.

Here we present the mathematical considerations necessary to develop estimates of the relative abundance of species in a barcoding sample of unknown individuals. The tools combine previous information on genetic diversity in the sampled population (that is, a species or population that has previously been analyzed at the same locus) with observed properties of the sample, such as the number of haplotypes and the number of segregating sites for each species. We then evaluate the situations in which there is sufficient power to make meaningful statements about relative abundance from polymorphism data alone. 

# Methods

```{r basic params, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}

# you may have biomass and/or estimate number indivs, e.g. could see ~200 zooplankton in a sample
# put 20, 200, 2000 indivs on NGS to barcode will not change proportion of reads, just changes depths of coverage
# but this info constrains the posterior in units of individuals rather than proportions
# especially if there are multiple taxa, in which the minimum number of each can be subtracted to set upper boundaries

maxindivs=200
```

Our approach is to identify information that can be used as prior information to establish the likelihood of observing polymorphism data from an *unknown* number of input individuals for a taxon. Any type of sampling information may help to set an upper limit: for example, if it is known that only `r maxindivs` individual specimens (of all taxa) were originally used for isolation of DNA, then the maximum number of total individuals inferred from this approach should be `r maxindivs` for any single included population. This itself is not a major advance in biology, but a limit on the inference nonetheless.

```{r hapdiv, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}

####################################
numhaps=10
Hapdiv=0.7 #minvalue 0.001 if no information; this is PRIOR information
####################################

library(plot3D)

x=1
cdf=0
indprob=0
array<-NULL
while (cdf<0.95) {
  cdfprev<-cdf
#  cdf<-pgamma(x,1,Hapdiv) 
#  if use 1 as shape parameter keeping shape parameter constant doesn't account for increased variance (?) as numhaps go up, 
#  e.g. error may be higher as you observe more...once it is working run it by somebody mathier.
  cdf<-pgamma(x,shape=(1/Hapdiv),rate=(1/numhaps)) 
# pgamma is the cumulative density function in R
#might be that numhaps is actually the shape parameter!!!! or: something else...non-gamma.
  indprob<-cdf-cdfprev
  
  happrob<-numhaps+(x-1)
  array<-c(array,happrob)
  array<-c(array,cdf)
  array<-c(array,indprob)
#  print(happrob)
#  print(cdf)
  x=x+1
}

#par(mfrow=c(3,1))

probs<-t(matrix(array,nrow=3))
probs
#plot(probs[,1],probs[,3],col='red')
maxhaps<-max(probs[,1])

```

There are also clear minimum bounds that can be established for the abundance of a taxon. Considering DNA sequence haplotypes as our most basic sample of genetic diversity, we ask how many *distinct* haplotypes are recovered in the data that match a particular taxon? For a haploid mitochondrial marker like the oft-applied cytochrome oxidase I (COI), this number is the minimum number of individuals present (if the number happens to be zero, it is also likely to be the maximum number of individuals in the sample!). 

We suggest three methods that could help to estimate the number of individuals for a particular species in a metabarcoding sample: 1) an inference based on prior estimates of haplotype diversity of a particular population and the observed *number of haplotypes* in a matched sample from that population, 2) an inference based on the expectations of Ewens' [-@Ewens1972] sampling theory given a prior estimate of $\theta$ and the *number of haplotypes* observed in the sample; and 3) an inference based on a prior estimate of $\theta$ in the field population and the observed *number of segregating sites* in the sample. 

To evaluate the potential usefulness of each method for recovering the abundance of input individuals, we simulated populations evolving under a Wright-Fisher neutral model. We performed the simulations with Hudson's *ms* program [@Hudson2002] using the *gap* [@gap2015] package in R [@RCoreTeam2015]. We simulated 3 populations, using three different population mutation rates ($\theta$ of 2, 10, and 20). For each population we then calculated summary statistics using the *PopGenome* [@PopGenome2014] package in R.

From the simulated populations (single simulations of 1000 alleles given $\theta$ of 2, 10 and 20) we took "field samples" of different sizes (*n* = 2, 4, 8, 16, 32, 64, 128), sampling without replacement. We replicate the sampling experiment 100 times for each combination of $\theta$ and *n*, to be able to assess variance associated with the sampling effort. For each replicate, we calculate the number of haplotypes and the number of segregating sites, which represent our observed values in the simulated samples. From these distributions of observed values (haplotypes and segregating sites) in each combination of theta and sampling size we selected the values within the percentiles 0.25 and 0.75 to use in back calculations of the number of individuals in that sample. The distributions are presented in the Supplementary Material (Figures S2 and S3). The sampling size, known to us from this design, is what we attempt to predict using the reversed inferences described below for each method. 

All the analysis of the simulated populations was done in R [@RCoreTeam2015], with the exception of the estimation of predicted segregating sites according to Wakeley [-@Wakeley08] that were performed in the software Mathematica (https://www.wolfram.com/mathematica/) given floating-point errors in R. Detailed information and the R code used to performed simulations is presented in the Appendix S1 in the Supplementary Material.

## Haplotype Diversity

In addition to the simple number of haplotypes observed at a barcode marker, we may also attempt to estimate the number of individuals that harbored those haplotypes. Here, we assume that there is previous information on haplotype diversity (*H*) from the natural populations of the species (or distinguishable populations) that are present in the barcoding sample. The "haplotype diversity", *H*, defined by Nei and Tajima (1981) as $$ H = \frac{N}{N-1}(1-\sum\limits_{i=1}x_{i}^2) $$ represents the probability that sampling a new individual will result in observation of a new haplotype. N is the number of haplotypes, and *x*~i~ is the sample frequency of the i~th~ haplotype.

An example of how *H* could be used is shown below for a sample in which `r numhaps` distinct haplotypes are observed,  and the *prior information about H* for a particular taxon is *H* = `r Hapdiv`. In addition to assuming that prior information about the population is appropriately comparable, here we assume a minimum of `r numhaps` individuals, and that what we do not know can be modeled by a Gamma distribution with the shape defined by the reciprocal of haplotype diversity (so that low diversity provides little information, high diversity suggests that the number of individuals is closer to the observed number of haplotypes), and the rate is defined by the reciprocal of the number of haplotypes.

```{r fig1, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}
plot(probs[,1],probs[,3],col='red',pch=19,xlab="Number of individuals (n)",ylab="Likelihood distribution of n")

max <- max(probs[,3])
maxPred <- probs[which(probs[,3] == max),1]
```

**Figure 1.** Likelihoods of identifying *n* individuals in a sample in which 10 haplotypes are observed, and the haplotype diversity of the originally observed population is *H* = 0.7. Here, a gamma function is applied to represent the likelihood such that the distribution is flat at low values of *H* and a sharper distribution with high values of *H*, bounded by the actual observed number of haplotypes.

So, observing `r numhaps` haplotypes for this taxon, and using the gamma to obtain a useful probability shape based on assumptions about how informative haplotype diversity is, we might feel comfortable believing `r maxPred` individuals were sampled (the highest likelihood solution). A concern here lies in the willful abuse of the gamma distribution without a better understanding of how haplotype diversity *H* and the sample size *N* may be actually related through the frequency of haplotypes - remember, at this point we are assuming we cannot trust the proportion/frequency representation of an allele in our sample.

For each of the 100 replicates in each sampling size within the three simulated populations we used the corresponding haplotype diversity for that population and the number of haplotypes observed in that replicate to estimate the likelihoods of the number of individuals in the sample (sampling size) using a gamma function as defined above (shape defined by the reciprocal of haplotype diversity and the rate defined by the reciprocal of the number of haplotypes). From each likelihood distribution we recorded the sampling size with the highest probability to compare with the simulated sampling size in each replicate. Finally, we calculated the difference between the "real" sampling size -the one from our simulations- and the sampling size inferred using the gamma distribution method, as a measure of the precision of our method. 

## Sampling theory 

Ewens [-@Ewens1972] developed a sampling theory of selectively neutral alleles, that based on the number of samples and the mutation parameter $\theta$, allows one to estimate the expected number of different alleles (here, we address alleles from a haploid genome, i.e. haplotypes) in a sample. Assuming a sample of n individuals, the mean number of haplotypes in a sample can be approximated by:

$$ E(h) = \frac{\theta}{\theta}+\frac{\theta}{\theta+1}+...+\frac{\theta}{\theta+n-1} $$

where,
*h* is the number of different haplotypes in the sample,
*n* is the number of individuals in the sample, and
$\theta$: 4N~e~u

If $\theta$ is very small, the expected number of haplotypes should be quite low regardless of the number of individuals sampled. On the other hand, if $\theta$ is extremely large, the number of haplotypes should tend to *n* as noted above; of course there is a close relationship between Ewens' sampling theory and our understanding of *H*. Using this equation, we can estimate the distribution of the number of haplotypes for different sampling sizes, with a variance:

$$ Var(h)= E(h)-[\frac{\theta^2}{\theta^2}+\frac{\theta^2}{(\theta+1)^2}+...+\frac{\theta^2}{(\theta+2n-1)^2}] $$

In general, the variance increases with $\theta$ for *n* of biological interest. Ewens' [-@Ewens1972] derivations rely on the assumption that the sample size is much lower than the actual population size. Considering this approach, rather than one based in haplotype diversity *H*, may allow us to avoid the problem of uncertain haplotype frequencies in an empirical data set.

For each of the three populations with theta equal to 2, 10 or 20, and through the range of sampling sizes considered in this study (2 to 128) we applied Ewens [-@Ewens1972] formula to estimate the expected number of haplotypes (and the variance) for each sampling size. We then compared the observed number of haplotypes in each sample with the expected number of haplotypes by Ewens's formula for each sampling size and estimated the "observed" sampling size in our sample. The accuracy of the method was calculated as the difference between the "real" sampling size - the one from our simulations - and the number of individuals for the sample estimated using Ewens's method.

## Segregating Sites

Under the standard coalescent model there are specific probability distributions associated with a sample of sequences, the number of segregating sites *S*, and a prior assumption of $\theta$ [@Wakeley08]:

$$ P(S=k) = \sum\limits_{i=2}^{n}(-1)^i{n-1 \choose i-1}\frac{i-1}{\theta+i-1}(\frac{\theta}{\theta+i-1})^k $$

Figure 2a illustrates this distribution for $\theta$=2, segregating sites *k* from 1 to 20, and number of individuals *n* from 1 to 20. This represents a low-diversity population, and unless few segregating sites are observed there may be a broad range of sample sizes consistent with such an observation. Figure 2b illustrates the same probability distribution, but assuming $\theta$=10, with a range of both *k* and *n* from 1 to 50. When the prior knowledge or assumption of diversity is higher, the range of segregating sites that could give a more accurate inference of *n* is higher, with a sharper distribution on *n* for lower to intermediate *k*.

```{r WakeleyCh4, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}

#install.packages("plot3D")
library(plot3D)

# ------theta = 2-------------
Q = 2 # this is PRIOR information from what you know of input SPECIES, not that particular data set (N. scab Q about 10)
maxn = 20 #above 70 this behaves funny???? OR IT MAY BE SOME MULTIPLE OF Q*maxn that is problem?

obsvdk = 4
maxk = 20 #must be greater than obsvdk

a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b

for (n in 2:maxn) {
#  print ("n")
#  print (n)
  for (k in 0:maxk) {
#    print (k)
    PSK=0
    for (i in 2:n) {
#      print (i)
      PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^k
      
    }
#    print (PSK)
    PSKv[k,n]<-PSKv[k,n]+PSK
    obsvd<-PSKv[obsvdk,]

  }
  
}

# ------theta = 10-------------
Q=10

maxn = 50 
obsvdk = 4
maxk = 50 #must be greater than obsvdk


a2 <- c(1:maxn)
b2 <- c(1:maxk)
PSKv2 <- numeric(maxn*maxk)
PSKv2 <- matrix(PSKv2,ncol=maxn)
colnames(PSKv2)<-a2
rownames(PSKv2)<-b2

for (n in 2:maxn) {

  for (k in 0:maxk) {

    PSK2=0
    for (i in 2:n) {

      PSK2<-PSK2 + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^k
      
    }

    PSKv2[k,n]<-PSKv2[k,n]+PSK2


  }
  
}

par(mfrow=c(1,1))

# ----make Wakeley plots--------
# NOTE: the "theta" command in the hist3D function is to define view angles and it is not related to the theta defined by us as mutation rate #

hist3D(x=seq(0,1,length.out=nrow(PSKv)), y=seq(0,1,length.out=ncol(PSKv)), PSKv, col=NULL, border=NA, theta=30,  phi=50, xlab="k", ylab="n", zlab="P{S=k}", alpha=0.35,colkey=FALSE)

#hist3D(x=seq(0,1,length.out=nrow(PSKv2)),y=seq(0,1,length.out=ncol(PSKv2)),PSKv2,col=NULL,border="black",theta=30,phi=50,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35)

hist3D(x=seq(0,1,length.out=nrow(PSKv2)),y=seq(0,1,length.out=ncol(PSKv2)),PSKv2,col=NULL,border=NA,theta=30,phi=50,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35,colkey=FALSE)


#par(mfrow=c(1,1))

```

**Figure 2.** Probability surface of observing a number of segregating sites *k* for a given sample size *n* when $\theta$ is set. In (a), $\theta$ = 2; in (b), $\theta$ = 10.

For each of the three populations with theta equal to 2, 10 or 20, and with a range of sampling sizes from 2 to 128, we applied Wakeley's [-@Wakeley08] equation 4.3 to estimate the expected number of segregating sites for each sampling size. We then compared the observed number of segregating sites in each sample with the expected number of segregating sites for each sampling size and estimated the "observed" sampling size in our sample. The accuracy of the method was calculated as the difference between the "real" sampling size - the one from our simulations - and the inferred number of individuals in the sample.

# Results 

The summary statistics of the simulated populations are presented in Table 1; the R code used to performed the simulations and estimate the summary statistics is presented in Appendix S1. As expected, the haplotype diversity, number of haplotypes and number of segregating sites increase with $\theta$. 

```{r Table 1 with info on simulated populations, warnings=F,message=FALSE, echo=FALSE,results='hide'}
#install.packages("gap")
#install.packages("knitr")

library(gap)
library(PopGenome)
library(knitr)

# reading the simulated populations files with PopGenome (returns a "genome" object)
readMS("theta2NoGrowth.out")->popgen.t2ng
readMS("theta10NoGrowth.out")->popgen.t10ng
readMS("theta20NoGrowth.out")->popgen.t20ng

# number of segregating sites----
varsites.t2ng<-popgen.t2ng@n.biallelic.sites
varsites.t10ng<-popgen.t10ng@n.biallelic.sites
varsites.t20ng<-popgen.t20ng@n.biallelic.sites

varsites<-c(varsites.t2ng,varsites.t10ng,varsites.t20ng)

# run F_ST stats and check haplotype diversity
F_ST.stats(popgen.t2ng)->popgen.t2ng
F_ST.stats(popgen.t10ng)->popgen.t10ng
F_ST.stats(popgen.t20ng)->popgen.t20ng

# get haplotype diversity for each source population----
unlist(popgen.t2ng@region.stats@haplotype.diversity)->hapDiv.t2ng
unlist(popgen.t10ng@region.stats@haplotype.diversity)->hapDiv.t10ng
unlist(popgen.t20ng@region.stats@haplotype.diversity)->hapDiv.t20ng
hapDiv<-c(hapDiv.t2ng,hapDiv.t10ng,hapDiv.t20ng)

# get haplotype counts----
length(unlist(popgen.t2ng@region.stats@haplotype.counts)/1000)->hapCount.t2ng
length(unlist(popgen.t10ng@region.stats@haplotype.counts)/1000)->hapCount.t10ng
length(unlist(popgen.t20ng@region.stats@haplotype.counts)/1000)->hapCount.t20ng
hapcounts<-c(hapCount.t2ng,hapCount.t10ng,hapCount.t20ng)

# summary table
thetas<-c(2,10,20)
pops<-c("Population 1","Population 2","Population 3")
#growth<-c("yes","no","yes","no","yes","no")
newhap<-round(hapDiv,2)
sumTable<-cbind(pops,thetas,newhap,hapcounts,varsites)
colnames(sumTable)<-c("Population","Theta","Haplotype diversity","Number of haplotypes","Number of segregating sites")

```

**Table 1**. Summary information on the simulated "field" populations that were used in this study.

```{r show table,echo=FALSE}
library(knitr)
kable(sumTable)
```

## Haplotype diversity and gamma estimation

Overall, using haplotype diversity (and our educated guess at how this diversity reflects the input) tends to greatly underestimate the simulated sample, at least for larger sampling size (Figure 3). For the smaller sampling sizes the difference between the predicted sampling size and the simulated sampling size is close to zero, meaning the method provides little error; the difference is also smaller when $\theta$ is larger (Figure 3). The probability distributions using this method and the predicted sampling sizes for each combination of theta and sampling size are provided in the supplementary material (Figure S3). 

```{r figure 3,message=FALSE, echo=FALSE} 
library(ggplot2)

# loading data
fullgammaData <- read.csv("gammaData_13Jul.csv",header=T) 

# subsetting data with growth
gammaData <- subset(fullgammaData,fullgammaData$Growth=="no")
droplevels(gammaData) -> gammaData

# rearrenging levels of theta for plots
gammaData$Theta <- factor(gammaData$Theta,levels=c("two","ten","twenty"))

# making sampling size a factor
factor(gammaData$samplingSize,levels=c(2,4,8,16,32,64,128)) -> gammaData$samplingSize.f

# subsetting the haplotypes for each combination that are between defined quantiles
thetas <- c("two", "ten", "twenty")
ns <- unique(gammaData$samplingSize)

mygammalist <- list()
for(i in thetas){
    for(j in ns){
      name <- paste(i,j)
      my.comb <- gammaData[which(gammaData$Theta == i & gammaData$samplingSize == j),]
      mode <- my.comb
      quant <- quantile(my.comb$n.haplotypes,c(0.75,0.25))
      names(quant) <- NULL
      keep <- subset(my.comb,my.comb$n.haplotypes >= quant[2] & my.comb$n.haplotypes <= quant[1])
      droplevels(keep) -> keep
      keep -> mygammalist[[name]]
    }
  }
  
my.good.gamma <- do.call("rbind", mygammalist)
row.names(my.good.gamma) <- NULL

my.good.gamma ->gammaData

# calculating the difference between "real" n and max gamma.
gammaData$samplingSize-gammaData$Max.Gamma.value -> gammaData$dif

# Figure 3
gamma.plot <- ggplot(gammaData, aes(x=samplingSize.f, y=dif,color=Theta))+ theme_bw() + geom_point(position = "jitter", alpha=0.3,aes(colour=Theta)) + geom_hline(yintercept = 0) + scale_alpha(guide = 'none') + xlab("Simulated sampling size") + ylab("Difference in simulated vs predicted sampling size") + ggtitle("Predictive method: Gamma distribution") + scale_colour_manual(values=c("darkgoldenrod1", "dodgerblue","maroon4"))

gamma.plot+ geom_segment(x=0.5,y=2,xend=1.5,yend=2,colour="red")+
            geom_segment(x=0.5,y=-2,xend=1.5,yend=-2,colour="red")+
            geom_segment(x=1.5,y=4,xend=2.5,yend=4,colour="red")+
            geom_segment(x=1.5,y=-4,xend=2.5,yend=-4,colour="red")+
            geom_segment(x=2.5,y=8,xend=3.5,yend=8,colour="red")+
            geom_segment(x=2.5,y=-8,xend=3.5,yend=-8,colour="red")+
            geom_segment(x=3.5,y=16,xend=4.5,yend=16,colour="red")+
            geom_segment(x=4.5,y=32,xend=5.5,yend=32,colour="red")+
            geom_segment(x=5.5,y=64,xend=6.5,yend=64,colour="red")+
            geom_segment(x=6.5,y=128,xend=7.5,yend=128,colour="red") +
            coord_cartesian(ylim = c(-10,130))
        
```

**Figure 3.** Difference between the simulated sampling size and the predicted sampling size using the gamma distribution method. The abline at zero shows the ideal situation with the estimation equal to the simulated sampling size. The red lines show the limit where the difference between simulated and predicted equals the sampling size, that is, beyond the red lines the error is too large to make a sensible prediction. Please note that the data are jittered to enhace visualization.

## Sampling theory

Using Ewens's sampling theory to estimate the number of individuals from the number of haplotypes gives a difference between the simulated and predicted sampling size that is centered to zero and that only has larger error for $\theta$ = 2 (Fig. 4 A,B). The precision of the method increases for larger values of $\theta$. 

```{r figure 4 sampling theory, echo=FALSE,message=FALSE}
library(ggplot2)

# -----observed data------
# ------------------------
# load data of number of haplotypes
hapdata <- read.csv("numberHaplotypes_10Sep2015.csv",header=T)
hapdata$Growth <- "no"

# subsetting data with growth
minidata <- subset(hapdata,hapdata$Growth == "no" & hapdata$Theta == "two"|hapdata$Theta == "ten"|hapdata$Theta == "twenty")
minidata[,-1] -> minidata
droplevels(minidata) -> minidata
row.names(minidata) <- NULL # cleaning row.names
#View(minidata)

# subsetting the haplotypes for each combination that are between defined quantiles
thetas <- c("two", "ten", "twenty")
ns <- unique(minidata$samplingSize)

myhaplist <- list()
for(i in thetas){
    for(j in ns){
      name <- paste(i,j)
      my.comb <- minidata[which(minidata$Theta == i & minidata$samplingSize == j),]
      quant <- quantile(my.comb$n.haplotypes,c(0.75,0.25))
      names(quant) <- NULL
      keep <- subset(my.comb,my.comb$n.haplotypes >= quant[2] & my.comb$n.haplotypes <= quant[1])
      droplevels(keep) -> keep
      keep -> myhaplist[[name]]
    }
  }
  
my.good.haps <- do.call("rbind", myhaplist)
row.names(my.good.haps) <- NULL



# -----predicted data------
#--------------------------
# load data of Ewens sampling theory
theory <- read.csv("dataSamplingTheory_10Sep2015.csv",header=T)
theory <- subset(theory, theory$theta.df == "two"|theory$theta.df == "ten"|theory$theta.df == "twenty")
theory <- droplevels(theory)

# round the expected number of haplotypes
round(theory$exp.haplotypes,0) -> theory$exp.round.hap

#---- merge predicted with observed data-----
#newdata <- merge(theory,minidata,by.x=c("theta.df","exp.round.hap"),by.y=c("Theta","n.haplotypes"),all.y=T)
newdata <- merge(theory,my.good.haps,by.x=c("theta.df","exp.round.hap"),by.y=c("Theta","n.haplotypes"),all.y=T)

#newdata[is.na(newdata$n.df),] # check NA's
newdata[complete.cases(newdata),]->newdata1

# calculate difference between Ewens n and the simulated n
newdata1$samplingSize-newdata1$n.df -> newdata1$difEwens

# rearrenging levels of theta for plots
newdata1$Theta <- factor(newdata1$theta.df,levels=c("two","ten","twenty"))

# making factor of sampling size 
factor(newdata1$samplingSize,levels=c(2,4,8,16,32,64,128)) -> newdata1$samplingSize.f

# dividing dataset in two
newdata2 <- newdata1[which(newdata1$samplingSize==2|newdata1$samplingSize==4|newdata1$samplingSize==8|newdata1$samplingSize==16),]
newdata3 <- newdata1[which(newdata1$samplingSize==32|newdata1$samplingSize==64|newdata1$samplingSize==128),]

ewens.plot <- ggplot(newdata2, aes(x=samplingSize.f, y=difEwens)) + theme_bw() + geom_point(position = "jitter",alpha=0.4,aes(colour=Theta)) + geom_hline(yintercept=0) + theme(legend.key = element_blank())+ scale_alpha(guide = 'none') + xlab("Simulated sampling size") + ylab("Difference in simulated vs predicted sampling size") + ggtitle("Predictive method: Ewens's sampling theory")+ scale_colour_manual(values=c("darkgoldenrod1", "dodgerblue","maroon4"))+ scale_y_continuous(limits = c(-20, 20))+facet_grid(Theta~.)

ewens.plot+ geom_segment(x=0.5,y=2,xend=1.5,yend=2,colour="red")+
         geom_segment(x=0.5,y=-2,xend=1.5,yend=-2,colour="red")+
         geom_segment(x=1.5,y=4,xend=2.5,yend=4,colour="red")+
         geom_segment(x=1.5,y=-4,xend=2.5,yend=-4,colour="red")+
         geom_segment(x=2.5,y=8,xend=3.5,yend=8,colour="red")+
         geom_segment(x=2.5,y=-8,xend=3.5,yend=-8,colour="red")+
         geom_segment(x=3.5,y=16,xend=4.5,yend=16,colour="red")+
         geom_segment(x=3.5,y=-16,xend=4.5,yend=-16,colour="red")
        
newdata3 <- newdata1[which(newdata1$samplingSize==32|newdata1$samplingSize==64|newdata1$samplingSize==128),]

ewens.plot2 <- ggplot(newdata3, aes(x=samplingSize.f, y=difEwens)) + theme_bw() + geom_point(position = "jitter",alpha=0.4,aes(colour=Theta)) + geom_hline(yintercept=0) + theme(legend.key = element_blank())+ scale_alpha(guide = 'none') + xlab("Simulated sampling size") + ylab("Difference in simulated vs predicted sampling size") + ggtitle("Predictive method: Ewens's sampling theory")+ scale_colour_manual(values=c("darkgoldenrod1", "dodgerblue","maroon4"))+ scale_y_continuous(limits = c(-130, 130))+facet_grid(Theta~.)

ewens.plot2 + geom_segment(x=0.5,y=32,xend=1.5,yend=32,colour="red")+
         geom_segment(x=0.5,y=-32,xend=1.5,yend=-32,colour="red")+
         geom_segment(x=1.5,y=64,xend=2.5,yend=64,colour="red")+
         geom_segment(x=1.5,y=-64,xend=2.5,yend=-64,colour="red")+
         geom_segment(x=2.5,y=128,xend=3.5,yend=128,colour="red")+
         geom_segment(x=2.5,y=-128,xend=3.5,yend=-128,colour="red")
```

**Figure 4.** Difference between the simulated sampling size and the sampling size for the sample predicted using Ewens's sampling theory. A) sampling sizes of 2,4,8 and 16; B) sampling sizes of 32, 64 and 128. The panels separate the three different values of $\theta$: two, ten and twenty. The vertical line at zero shows the ideal situation in which the estimation equals the simulated sampling size. The red lines show the limit where the difference between simulated and predicted equals the sampling size, that is, beyond the red lines the error is too large to make a sensible prediction. Please note that the results are jittered to enhance visualization.

## Theta and segregating sites approach

The implementation of Wakeley's [-@Wakeley08] formula for estimation of the relationship between the number of segregating sites *k*, the number of individuals *n* and $\theta$ presented the larger differences of precision between populations with different $\theta$. The estimation of *n* for $\theta$=20 are totally flawed, with predicted values that overestimate the real *n* and errors that are far larger than the sampling size (Fig. 5). The estimations of *n* tend to be understimated in $\theta$=2 and overestimated in $\theta$=10, in particular for smaller sampling sizes (Fig. 5).

```{r plotting Wakeley data NEW DATA,echo=FALSE,message=FALSE,warning=FALSE}
library(ggplot2)
library(lattice)
library(plyr)
library(modeest)

# we have two types of data:
# "WakeleyData" with results of Wakeley formula for all k and n combinations
# "segsites" has the observed number of seg.sites in the simulations
wakdata<-read.csv("Wakeley_Toby_30Sep2015.csv",header=T)
segsites<-read.csv("numberSegSites_10Sep2015.csv",header=T)

# making factors for the ggplot2 plots
factor(wakdata$wak.segsites)->wakdata$wak.segsites.f
factor(segsites$samplingSize)->segsites$samplingSize.f

# -----------------------------------
# ------observed segsites------------
# -----------------------------------
segsites[,-1] -> segsites
segsites$Pop <- NULL
segsites$Growth <- NULL

# estimate the mode (the more frequent value) for each combination of sampling size and theta
# the mode is estimated with the function mvl in the package "modeest"
# the warning is because there are multiple modes and I'm collapsing them with the "unlist"
#modes <- ddply(segsites, c("Theta","samplingSize.f"), summarise, mode=unlist((mlv(n.seg.sites))[1]))
#modes$samplingSize <- as.numeric(as.character(modes$samplingSize.f))

# subsetting the segregating sites for each combination that are between defined quantiles
thetas <- c("two", "ten", "twenty")
ns <- unique(segsites$samplingSize)

mylist <- list()
for(i in thetas){
    for(j in ns){
      name <- paste(i,j)
      my.comb <- segsites[which(segsites$Theta == i & segsites$samplingSize == j),]
      quant <- quantile(my.comb$n.seg.sites,c(0.75,0.25))
      names(quant) <- NULL
      keep <- subset(my.comb,my.comb$n.seg.sites >= quant[2] & my.comb$n.seg.sites <= quant[1])
      droplevels(keep) -> keep
      keep -> mylist[[name]]
    }
  }
  
my.good.segs <- do.call("rbind", mylist)
row.names(my.good.segs) <- NULL

# -------------------------------------
# -------Wakeley's predictions --------
# -------------------------------------

# estimate the best n's of wakeley data
bestprobs <- ddply(wakdata, c("Theta","wak.segsites.f"), summarise, best.prob=max(wak.prob))

# merging "bestprobs" with "wakdata" to recover the n's that match with the higher probability
best.n <- merge(bestprobs,wakdata,by.x=c("Theta","wak.segsites.f","best.prob"),by.y=c("Theta","wak.segsites.f","wak.prob"),all.x=T)

# Now we want to merge the observed segregating sites with the segregating sites used to estimate Wakeley's formula, and that will allow us to plot the "real" n vs the best.n according Wakeley.
#allwak <-merge(modes,best.n,by.x=c("Theta","mode"),by.y=c("Theta","wak.segsites"))
allwak <- merge(my.good.segs,best.n,by.x=c("Theta","n.seg.sites"),by.y=c("Theta","wak.segsites"))

# rearrenging levels of theta for plots
allwak$Theta <- factor(allwak$Theta,levels=c("two","ten","twenty"))

# Calculate difference between real and predicted n
allwak$dif <- allwak$samplingSize - allwak$wak.n

myplot <- ggplot(allwak, aes(x=samplingSize.f, y=dif,color=Theta,alpha=0.2)) + theme_bw()+geom_point()+geom_point(position = "jitter",aes(colour=Theta))+geom_hline(yintercept=0)+xlab("Simulated sampling size")+ylab("Difference in simulated vs predicted sampling size") + ggtitle("Predictive method: Wakeley's likelihood") + scale_alpha(guide = 'none') + scale_y_continuous(limits = c(-150, 150))+ scale_colour_manual(values=c("darkgoldenrod1", "dodgerblue","maroon4"))

myplot + geom_segment(x=0.5,y=2,xend=1.5,yend=2,colour="red")+
         geom_segment(x=0.5,y=-2,xend=1.5,yend=-2,colour="red")+
         geom_segment(x=1.5,y=4,xend=2.5,yend=4,colour="red")+
         geom_segment(x=1.5,y=-4,xend=2.5,yend=-4,colour="red")+
         geom_segment(x=2.5,y=8,xend=3.5,yend=8,colour="red")+
         geom_segment(x=2.5,y=-8,xend=3.5,yend=-8,colour="red")+
         geom_segment(x=3.5,y=16,xend=4.5,yend=16,colour="red")+
         geom_segment(x=3.5,y=-16,xend=4.5,yend=-16,colour="red")+
         geom_segment(x=4.5,y=32,xend=5.5,yend=32,colour="red")+
         geom_segment(x=4.5,y=-32,xend=5.5,yend=-32,colour="red")+
         geom_segment(x=5.5,y=64,xend=6.5,yend=64,colour="red")+
         geom_segment(x=5.5,y=-64,xend=6.5,yend=-64,colour="red")+
         geom_segment(x=6.5,y=128,xend=7.5,yend=128,colour="red")+
         geom_segment(x=6.5,y=-128,xend=7.5,yend=-128,colour="red")
```

**Figure 5.** Difference between the simulated sampling size and the sampling size for the sample predicted using Wakeley's segregating sites theory. The line at zero shows the ideal situation in which the estimation equals the simulated sampling size. The red lines show the limit where the difference between simulated and predicted equals the sampling size, that is, beyond the red lines the error is too large to make a sensible prediction. Please note that the data are jittered to enhace visualization. *HERE VALUES FOR SAMPLES OF 64, 128 PRESENT ERROR FOR THETA OF 20 BECAUSE OF CALCULATION LIMIT IN MATHEMATICA; WE WILL FIX THIS*

# Discussion

What we have shown is, in effect, the high variance in genealogical and mutational data associated with the coalescent process in population genetics [@Kingman82]. Though our early efforts suggested a broad utility in ranking the abundance of taxa in a mixed sample of metabarcode data, the result of our extended simulations indicate a preponderance of high-variance, downward-biased results in estimating the number of individuals in a sequence data set. In considering basic haplotype diversity *H*, the observed number of haplotypes, as well as the number of segregating sites *S*, our attempts to use genetic diversity tend to greatly underestimate the simulated sample of individuals, at least for larger sampling sizes and/or low values of $\theta$. Fundamentally, this problem can be seen in Figure 2 for $\theta$=10; although there is an observable and relatively sharp distribution of probability relating *k* and *n*, the surface is quite wide with respects to *n* as *k* increases for a given $\theta$. 

The one component of these summary statistics that indicates an unbiased relationship between $\theta$, *n*, and the number of haplotypes is, of course, a simple reflection of Ewens' [-@Ewens1972] sampling theory. This suggests that *if* we already have a good understanding of $\theta$ for a particular population (for example if metabarcoding efforts are documenting the mix of individuals in a system that is already well-characterized for most species), then observing a number of haplotypes in a metabarcode sample allows some nominal indication of how many individuals are likely to have been included. Overall, however, if our goal is to improve the ability to quantitatively describe the biodiversity in a system using metabarcoding approaches, we show that such an approach is of poor utility unless the diversity of the system is high and the number of individuals input to the metabarcoding analysis is modest. Given the additional uncertainty associated with assumptions of comparable diversity from prior evaluation of each population, there are no benefits in cost or estimation over traditional barcoding of individual specimens.

Some of the error or bias in estimation we note from our simulation work reflects common problems in sampling data and exploring them with summary statistics. Felsenstein [-@Felsenstein92] had noted that a high variance (in fact, driven by bimodal distribution of resultant statistics) in the number of segregating sites *S* would be expected with low sample size. Effectively, the comparison of a small number of sequences in a high-diversity system has a large probability of pairwise contrasts across the oldest node of a genealogy [@Felsenstein92], and at very small sample sizes there is a potential that two closely-related sequences are sampled rather than reflect the TMRCA of the genealogy. Additionally, the saturating relationship between sample size and observed diversity has been pointed out by Wakeley [-@Wakeley08] as a feature of efficient estimation of $\theta$ from a natural sample; turning the crank on this equation backwards results in an inefficient inference of *n* from the other parameters.

Additionally, our approach is predicated on the idea that prior analysis of a given population - a genetically discrete and relatively homogeneous evolutionary unit - will effectively suggest the diversity to be found in subsequent samples. There are certainly instances where the diversity at a barcode locus has been so extraordinarily high that haplotype diversity approached 1, and the number of haplotypes recovered in a sample was very close to the number of individuals in that sample, such as the barnacle *Balanus glandula* [@Wares01,@Sotka04,@Wares05]. However, this same example of a hyperdiverse barnacle also requires recognition that there are at least 2 distinct evolutionary lineages in this taxon with broadly overlapping geographic ranges [@Sotka04,@Wares05], which dramatically affects our understanding of the diversity recovered as well as the underlying genealogical process and association with regional diversity.

The statistics we evaluate are not independent from one another; they pertain to the same genealogical process assumed to underlie a sample of DNA sequence data and are different ways of summarizing this coalescent process. Although some methods, e.g. approximate Bayesian computation, have been used to infer the demographic history of a sample of sequences using the aggregate of summary statistics available (e.g. Ilves et al. 2010), the relationship shown here appears to be too tenuous to make an advance in our ability to estimate relative abundance of taxa from such metabarcode data. It does seem that among high-$\theta$ populations there may still be comparisons appropriate in a relative sense: greater haplotypic diversity from a metabarcode sample would suggest more individuals of that species were in the sample. In this way we can evaluate order-of-magnitude results, and have less need for prior information from a population. From the metabarcoding data themselves, each discrete population offers an estimate of $\theta$ and a number of observed, distinct haplotypes; this information is likely sufficient to bin abundances into more inclusive groupings of "common", "intermediate", and "rare" (e.g. a simplified Preston or Whittaker plot; [@Magurran2004]).

This leaves metabarcode research with three options: (1) continue to individually sequence using Sanger methods; (2) only use metabarcode data for presence/absence of a taxon; (3) in cases where the amplification bias may be considered negligible, as with closely-related lineages, the frequency of reads may be useful for approximating the *relative* but not absolute abundance of lineages in a sample. Though there are concerns about how well read/sequence frequency reflects the relative abundance of populations in an environmental sample - driven largely by differential amplification success of target genomes [@Nguyen14; @Pinol14; @Bohmann14], it is worth noting that at least one recent study found a strong correlation between the proportion of metabarcoding reads from a taxon and the relative abundance of that taxon given point counts [@Leray15], as well as a strong correlation with taxon frequency from individual barcode data. If this relationship is supported in further studies, and if we can use information such as the number of haplotypes in a taxon as complementary information for minimum abundance, then we may start to improve on our ability to recover actual ecology from actual molecules.

# Acknowledgments

Idea brought about by extended problem-solving session with J. Drake and colleagues in the Odum School of Ecology and Department of Genetics at The University of Georgia (UGA). Our work was improved greatly by suggestions from Ana Bento, C. Ewers-Saucedo and K. Bockrath. We thank Toby Brett for his help to code in Mathematica. This work was supported by funding from NSF (OCE-1029526) and the UGA Department of Genetics.

# Figure captions

# Supporting information 

**Appendix S1.** R code used to perform simulations and to estimate genetic diversity indexes.  

## Supplementary figures

**Supplementary Figure 1.** Distribution of the number of haplotypes for each combination of theta (two, ten, twenty) and sampling size (2, 4, 8, 16, 32, 64, 128). 

```{r histogram of haplotypes,echo=FALSE,message=F,warning=FALSE}
# load data from simulations
fullgammaData <- read.csv("gammaData_13Jul.csv",header=T) 

# subsetting data with growth
gammaData <- subset(fullgammaData,fullgammaData$Growth=="no")
droplevels(gammaData) -> gammaData

# rearrenging levels of theta for plots
gammaData$Theta <- factor(gammaData$Theta,levels=c("two","ten","twenty"))

# making sampling size a factor
factor(gammaData$samplingSize,levels=c(2,4,8,16,32,64,128)) -> gammaData$samplingSize.f

dist.hap<-ggplot(gammaData, aes(x = n.haplotypes)) + theme_bw() + geom_histogram() + facet_grid(Theta~samplingSize.f) + xlab("Number of haplotypes") + ylab("Frequency")
dist.hap
```

**Supplementary Figure 2.** Distribution of the number of segregating sites for each combination of theta (two, ten, twenty) and sampling size (2, 4, 8, 16, 32, 64, 128). 

```{r histogram of seg sites,echo=FALSE,message=F,warning=FALSE}
# load data from simulations
segsites<-read.csv("numberSegSites_10Sep2015.csv",header=T)

# making factors for the ggplot2 plots
factor(segsites$samplingSize)->segsites$samplingSize.f

segsites[,-1] -> segsites
segsites$Pop <- NULL
segsites$Growth <- NULL
segsites1 <- segsites[which(segsites$Theta=="two"|segsites$Theta=="ten"|segsites$Theta=="twenty"),]
droplevels(segsites1) -> segsites2

# rearrenging levels of theta for plots
segsites2$Theta <- factor(segsites2$Theta,levels=c("two","ten","twenty"))

dist.seg<-ggplot(segsites2, aes(x = n.seg.sites)) + theme_bw() + geom_histogram() + facet_grid(Theta~samplingSize.f) + xlab("Number of segregating sites") + ylab("Frequency")
dist.seg
```


**Supplementary Figure 3.** Probability distribution of the number of individuales back calculated using the gamma approach. Each panel shows a combination of simulated sampling size and the mutation rate in the original populations.  

```{r SuppFig3 Gamma probs,echo=FALSE,message=F,warning=FALSE}
library(ggplot2)

# load data with number of haplotypes
hapnum<-read.csv("numberHaplotypes_10Sep2015.csv",header=T)
hapnum$X<-NULL

# subsetting only theta 2, 10, 20
hapnum1 <- hapnum[which(hapnum$Theta=="two"|segsites$Theta=="ten"|segsites$Theta=="twenty"),]
droplevels(hapnum1) -> hapnum2

# Haplotype diversity for each of the six simulated populations of 1000 individuals
hapdiv <- c(0.3377538, 0.9258679, 0.9565626)

# add the column of haplotype diversity for each population
for(i in 1:3){
hapnum2$hap.diversity[hapnum2$Pop == i] <- hapdiv[i] 
}

df.list <- list()
for(i in 1:nrow(hapnum2)){
  #print(paste("working in row number",i))
    mydf<- data.frame(theta=NA,n=NA,hap.num=NA,p=rep(NA,500)) 
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,shape=(1/hapnum2$hap.diversity[i]),rate=(1/hapnum2$n.haplotypes[i]))
                  
      indprob<-cdf-cdfprev
      happrob<-hapnum2$n.haplotypes[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
    probs<-t(matrix(array,nrow=3))
    
    # fill dataframe
    for (j in 2:nrow(probs)){
    mydf$theta[j] <- as.character(hapnum2$Theta[i])
    mydf$p[j] <- probs[j,3]
    mydf$n[j] <- probs[j,1]
    mydf$samplingSize[j] <- hapnum2$samplingSize[i]
    mydf$hap.num[j] <- hapnum2$n.haplotypes[i]
        }
  mydf -> df.list[[i]]  
  }

# make dataframe from list
mydata1 <- do.call("rbind", df.list)
mydata2 <- mydata1[complete.cases(mydata1$n),]
row.names(mydata2) <- NULL

# add factor to plot
as.factor(mydata2$samplingSize) -> mydata2$samplingSize.f
as.factor(mydata2$theta) -> mydata2$theta.f

# rearrenging levels of theta for plots
mydata2$theta.f <- factor(mydata2$theta.f,levels=c("two","ten","twenty"))


# make plots
# to have the plot with free scales
ggplot(mydata2, aes(x=n, y=p))+ geom_point()+theme_bw() + facet_grid(theta.f ~ samplingSize.f,scales="free") + xlab("Number of individuals") + ylab("Probability")

#View(mydata2[which(mydata2$theta=="two" & mydata2$hap.num==4),])
```


# Literature Cited


