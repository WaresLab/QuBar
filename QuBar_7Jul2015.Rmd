---
title: "Quantitative Metazoan Metabarcoding"
author: "John Wares and Paula Pappalardo"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: word_document
bibliography: QuBar_References.bib
---

# Introduction

We start with what may seem like a trivial question: assume that you have been told that a series of fair coin flips resulted in 60% 'heads', 40% 'tails'. This is the only information given, but you already have made a judgment about how many coin flips occurred, and perhaps have generated a probability distribution in your head where the highest likelihood is for 5 or 10, rather than 50 or 100, events. This is taking advantage of what we know about the probability mass function of a binomial distribution, where the observed number of 'successes' in a series is related to the probability of success (presumably 50%) and the number of trials.  

Here, we consider whether the same principle could be used for improving the efficiency of exploring the presence, distribution, and abundance of genetic biodiversity. Documenting the distribution and abundance of biodiversity - in many habitats, at multiple scales - is perhaps more important now than ever as scientists evaluate how populations are responding to environmental change. Though technological advances have rapidly improved some elements of this [@Nagendra01; @Bourlat13], there are still glaring deficiencies in our ability to efficiently catalog diversity, even in small domains or limited taxonomic surveys.

The most apparent advances have been in surveys of microbial and viral diversity. Next-generation sequencing has permitted the now-commonplace exploration of fungal, bacterial, and viral diversity by generating $10^5$ - $10^6$ sequence reads per sample and using barcoding approaches (match of sequence to known taxonomic samples for that genomic region) to identify the taxa present and their relative abundance. While there is no doubt that this has transformed our understanding of functional ecosystem processes and microbial ecology at this scale [@Nguyen14; @Turn09; @Desnues08], there are definite limitations. For example, some taxa (e.g. Archaea) may not be as readily amplified using the same ribosomal 16S "bacteria" primers, and variation in amplification efficiency certainly exists within the Eubacteria [@Acinas05]. Additionally, it is known that some bacterial genomes harbor more than one copy of this canonical locus [@Kembel12], thus muddling the relationship between read frequency and taxon frequency in a community.

The same problems exist - and are exacerbated - when studying multicellular diversity. Most notably, on top of the problems of potential contamination, detecting rare taxa and/or handling singleton evidence for rare taxa, and the potentially large variance in individual sizes of organisms, the relative read abundance in a NGS data set will often wildly vary (by multiple orders of magnitude) from the abundance of actual tissue in the data set [@Nguyen14; @Pi√±ol14; @Bohmann14]. This is caused primarily by shifts in amplification efficiency given mismatches in the primer region, and is often dealt with by analyzing data for simple incidence as well as relative read abundance, to identify patterns robust to either removal of information or inaccurate information [@Nguyen14].

If, however, our goal is to understand the actual relative abundance of individuals of different species in a sample - with these species harboring variation at 'barcode' loci, and often being highly divergent from one another - the question is whether there is complementary information that can be extracted from these data that does not rely on the abundance of reads that are assigned to a taxon, but relies on our understanding of diversity within populations and how that can be measured. 

The summary statistics for DNA sequence diversity are well established and generally recognize the population mutation rate $\theta$ at a given locus; as a population increases in size, or as the mutation rate at that locus increases, more polymorphisms and more diversity will be found. There are limitations to this approach based on Kimura's neutral theory, as various forms of genomic selection will limit the direct relationship between population size and population diversity [e.g., @Bazin06; @Wares10; @Corbett15]. Nevertheless, these summary statistics - including Watterson's $\theta$, a sample-normalized estimator of $\theta$ using the number of segregating sites *S* in a sample - may provide information necessary to generate *some* information about abundance patterns from NGS data. This information also certainly has its limits: nucleotide diversity ($\pi$) requires information on polymorphic site frequencies that will be biased by differential amplification across individuals, as well as relatively uninformative - or diminishing returns - as the number of sampled individuals increases [@Wakeley08]. Haplotype diversity (*H*) is likely sufficient to set a minimum boundary on the number of individuals sampled, and *H* along with *S* have some information about the probability associated with larger numbers of individuals.

Here we present the mathematical considerations necessary to develop these quantitative tools, and then evaluate the situations in which there is sufficient power to make meaningful statements about relative abundance from polymorphism data alone. 

# Methods

```{r basic params, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}

# you may have biomass and/or estimate number indivs, e.g. could see ~200 zooplankton in a sample
# put 20, 200, 2000 indivs on NGS to barcode will not change proportion of reads, just changes depths of coverage
# but this info constrains the posterior in units of individuals rather than proportions
# especially if there are multiple taxa, in which the minimum number of each can be subtracted to set upper boundaries

maxindivs=200
```

The approach here is identifying information that can comfortably be used as prior information to establish the posterior probability of observing polymorphism data from an *unknown* number of input individuals for a taxon. Any type of sampling information may help to set an upper limit: for example, if it is known that only about `r maxindivs` individual specimens were originally used for isolation of DNA, then the maximum number of total individuals inferred from this approach should be about `r maxindivs`. This is perhaps not a groundbreaking numerical advance in biology, but limits our prior belief nonetheless.

```{r hapdiv, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}

####################################
numhaps=10
Hapdiv=0.7 #minvalue 0.001 if no information; this is PRIOR information
####################################

x=1
cdf=0
indprob=0
array<-NULL
while (cdf<0.95) {
  cdfprev<-cdf
#  cdf<-pgamma(x,1,Hapdiv) 
#  if use 1 as shape parameter keeping shape parameter constant doesn't account for increased variance (?) as numhaps go up, 
#  e.g. error may be higher as you observe more...once it is working run it by somebody mathier.
  cdf<-pgamma(x,shape=(1/Hapdiv),rate=(1/numhaps)) #might be that numhaps is actually the shape parameter!!!! or: something else...non-gamma.
  indprob<-cdf-cdfprev
  
  
  happrob<-numhaps+(x-1)
  array<-c(array,happrob)
  array<-c(array,cdf)
  array<-c(array,indprob)
#  print(happrob)
#  print(cdf)
  x=x+1
}

#par(mfrow=c(3,1))

probs<-t(matrix(array,nrow=3))
probs
#plot(probs[,1],probs[,3],col='red')
maxhaps<-max(probs[,1])


```

There are also clear minimum bounds that can be established for the abundance of a taxon. Considering DNA sequence haplotypes as our most basic information, we ask how many *distinct* haplotypes are recovered in the data that match a particular taxon? For a haploid mitochondrial marker like the oft-applied cytochrome oxidase I (COI), this number is the minimum number of individuals present (if the number happens to be 0, it is also likely to be the maximum number of individuals in the sample). 

## Gamma approach

In addition to the simple number of haplotypes observed at a barcode marker, we may also attempt to estimate the number of individuals that harbored those haplotypes. Here, we assume that there is previous information on haplotype diversity (*H*) from the natural populations of the species (or distinguishable populations) that are present in the barcoding sample. The "haplotype diversity", *H*, defined by Nei and Tajima (1981) as $$ H = \frac{N}{N-1}(1-\sum\limits_{i=1}x_{i}^2) $$ represents the probability that sampling a new individual will result in sample of a new haplotype. N is the number of haplotypes, and *x*~i~ is the sample frequency of the i~th~ haplotype.

An example of how *H* could be used is shown below for a sample in which `r numhaps` haplotypes are observed,  and the *prior information about H* for a particular taxon is *H* = `r Hapdiv`. In addition to assuming that prior information about the population is useful, here we assume a minimum of `r numhaps` individuals, and that what we do not know can be modeled by a Gamma distribution with the shape defined by the reciprocal of haplotype diversity (so that low diversity provides little information, high diversity suggests that the number of individuals is closer to the observed number of haplotypes), and the rate defined by the number of haplotypes.

```{r fig1, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}
plot(probs[,1],probs[,3],col='red',pch=19,xlab="Number of individuals (n)",ylab="Probability distribution of n")

```

So, observing `r numhaps` haplotypes for this taxon, and given a relatively haphazard use of the Gamma to obtain a useful probability shape given assumptions about how informative haplotype diversity is, we might feel comfortable believing (with a 95% interval) there are between 
`r numhaps` and `r maxhaps` actual individuals that were sampled, with a most-probable solution of 14. A problem lies in the willful abuse of the Gamma distribution without a better understanding of how haplotype diversity *H* and the sample size *N* are related through the frequency of haplotypes - remember, at this point we are assuming we cannot trust the proportion/frequency representation of an allele in our sample.

## True diversity indices

Another way to approach this is through 'true diversity' indices [reviewed in @Sherwin10], as this family of statistics indicates the number of equally abundant types that have an average frequency equal to the observed average of types. This would mean that instead of taking H as a given from a previously-studied taxon or population, the extant data would be used to calculate $$ ^qD = (\sum\limits_{i=1}^{R}p_{i}^q)^{1/(1-q)} $$ where *R* is richness (number of types). As the Simpson (1949) index is used to indicate the probability that two individuals taken at random from the data are of the same (haplo)type, a comparable statistic to *H* is reached by taking the inverse Simpson index, or $^2D$, and then transforming into the Gini-Simpson index as $1-1/^2D$, again the probability that two individuals have distinct haplotypes.

```{r NotoPrior, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}
# rough estimates from Zakas Q is 10, hapdiv is 0.7
# grabbing a file from Geneious....
#install.packages("PopGenome")
library(PopGenome)
file<-readData("FastaSeqs") # actual sample size here is 20. That is the number I'd like to come out...AND, TO AN EXTENT, IT IS AT LEAST IN THE INTERVALS FOR BOTH...
#actual<-20
file@n.sites
basic<-diversity.stats(file)
Hapdiv<-basic@hap.diversity.within #haplotype diversity

Nsite<-basic@n.biallelic.sites #number of seg sites taht are biallelic, for now assume ISM
more<-basic@n.polyallelic.sites
varsite<-Nsite+more

filehaps <- F_ST.stats(file,mode="haplotype",only.haplotype.counts=TRUE)
haplotypecounts <- filehaps@region.stats@haplotype.counts
# this is helpful https://github.com/cran/PopGenome/blob/master/vignettes/Integration_of_new_Methods.Rnw 

########

#install.packages("entropart")
library(entropart)
#need frequencies of haplotypes reported from PopGenome so you can use entropart to get 2D
hapfreq<-unlist(haplotypecounts)
actual<-sum(hapfreq)
hapfreq<-hapfreq/actual
numhaps<-length(hapfreq)

Simp<-expq(Simpson(Ps=hapfreq),q=2)
Gini<-1-(1/Simp)

# Watterson estimator of theta
harm=0
for (n in 1:(actual-1)){
  harm=harm+1/n
}

QW <- varsite/harm

```

```{r hapdivexample, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}

x=1
cdf=0
indprob=0
array<-NULL
while (cdf<0.95) {
  cdfprev<-cdf
  cdf<-pgamma(x,shape=(1/Hapdiv),rate=(1/numhaps)) 
  indprob<-cdf-cdfprev
  
  
  happrob<-numhaps+(x-1)
  array<-c(array,happrob)
  array<-c(array,cdf)
  array<-c(array,indprob)
  x=x+1
}
min=numhaps
max=x
probs<-t(matrix(array,nrow=3))
probs
#plot(probs[,1],probs[,3],col='red')

```


Evaluating a sample of sequences from the barnacle *Notochthamalus scabrosus*, where `r actual` individuals were haphazardly sampled from the data of Laughlin et al. [-@Laugh12], we see that these data would traditionally report haplotype diversity *H* of `r Hapdiv`, from `r numhaps` observed haplotypes (most dominant haplotype at frequency `r max(hapfreq)`), and in this instance the Gini-Simpson index is equal to `r Gini`. Now we have another statistic that can be calculated from previous data on the population, that focuses on the number of dominant haplotypes. Here, the inverse Simpson index $^2D$ is `r Simp`, the effective number of haplotypes in the system. Using the approach detailed above, but given these data and assuming that the number of input sequences is unknown, we could say there are between `r min` and `r max` individuals that went into the sample. However, the method by which haplotype diversity is calculated does not address the guesswork by which we have so far applied our probability model.

This model is intuitively useful - the number of individuals cannot be less than the number of haplotypes, and is likely more; the number of individuals could be much more than the number of haplotypes if marker diversity is low; and the distribution scales up as the minimum increases - however is mathematically not well-informed. 

## Theta and segregating sites (Wakeley 2008)

As noted above, there are specific probability distributions associated with a sample of sequences, the number of segregating sites *S*, and a prior assumption of $\theta$ [@Wakeley08].

$$ P(S=k) = \sum\limits_{i=2}^{n}(-1)^i{n-1 \choose i-1}\frac{i-1}{\theta+i-1}(\frac{\theta}{\theta+i-1})^k $$

Figure Xa illustrates this distribution for $\theta$=2. This represents a low-diversity population, and unless few segregating sites are observed there may be a broad range of sample sizes consistent with such an observation. Figure Xb illustrates the same probability distribution, but assuming $\theta$=10. When the prior knowledge or assumption of diversity is higher, there tends to be a sharper distribution on *n* for a given *k*.


```{r WakeleyCh4, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}

#install.packages("plot3D")
library(plot3D)
Q = 2 # this is PRIOR information from what you know of input SPECIES, not that particular data set (N. scab Q about 10)
maxn = 20 #above 70 this behaves funny???? OR IT MAY BE SOME MULTIPLE OF Q*maxn that is problem?

obsvdk = varsite
maxk = 15 #must be greater than obsvdk



a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b

for (n in 2:maxn) {
#  print ("n")
#  print (n)
  for (k in 0:maxk) {
#    print (k)
    PSK=0
    for (i in 2:n) {
#      print (i)
      PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^k
      
    }
#    print (PSK)
    PSKv[k,n]<-PSKv[k,n]+PSK
    obsvd<-PSKv[obsvdk,]

  }
  
}

Q=10
a2 <- c(1:maxn)
b2 <- c(1:maxk)
PSKv2 <- numeric(maxn*maxk)
PSKv2 <- matrix(PSKv2,ncol=maxn)
colnames(PSKv)<-a2
rownames(PSKv)<-b2

for (n in 2:maxn) {

  for (k in 0:maxk) {

    PSK2=0
    for (i in 2:n) {

      PSK2<-PSK2 + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^k
      
    }

    PSKv2[k,n]<-PSKv2[k,n]+PSK2


  }
  
}


par(mfrow=c(1,2))

hist3D(x=seq(0,1,length.out=nrow(PSKv)),y=seq(0,1,length.out=ncol(PSKv)),PSKv,col=NULL,border="black",theta=30,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35)

hist3D(x=seq(0,1,length.out=nrow(PSKv2)),y=seq(0,1,length.out=ncol(PSKv2)),PSKv2,col=NULL,border="black",theta=30,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35)

par(mfrow=c(1,1))

```

## Sampling theory 

Ewens [-@Ewens1972] developed the sampling theory of selectively neutral alleles, that based in the number of samples and the mutation parameter $\theta$, allows one to estimate the expected number of different alleles (here, we address alleles from a haploid genome, i.e. haplotypes) in a sample. Assuming a sample of n individuals, the mean number of haplotypes in a sample can be approximated by:

$$ E(h) = \frac{\theta}{\theta}+\frac{\theta}{\theta+1}+...+\frac{\theta}{\theta+n-1} $$

where,
*h* is the number of different haplotypes in the sample,
*n* is the number of individuals in the sample, and
$\theta$: 4N~e~u

If $\theta$ is very small, the expected number of haplotypes should approximate 1, on the other hand, if $\theta$ is extremely large, the number of haplotypes should tend to *n* as noted above. Using this equation, we can estimate the distribution of the number of haplotypes for different sampling sizes, with a variance:

$$ Var(h)= E(h)-[\frac{\theta^2}{\theta^2}+\frac{\theta^2}{(\theta+1)^2}+...+\frac{\theta^2}{(\theta+2n-1)^2}] $$

In general, the variance increases with theta for *n* of biological interest. Ewens [-@Ewens1972] derivations rely on the assumption that the sampling size is much lower than the actual population size.

#Evaluation of Inferences

We propose two/three methods that could help to estimate the relative number of individuals in a barcoding sample: 1) gamma distribution, 2) theta and segregating sites, 3) sampling theory?. To evaluate the usefulness of each method, we used a simulation approach. We simulated populations with Hudson's MS program [@Hudson2002] using the *gap* package in R [@RCoreTeam2013]. We simulated four populations, using two/three? different mutation rates (theta= 2 and theta=10), and also considering populations with or without growth. The growth rate was modified in order to achieve a Tajima's D value that better approximates the values observed in nature for marine invertebrates. 

From these four populations, we took "field samples" of different sizes (n), sampling without replacement. We replicate the sampling experiment 100 times, to be able to assess variation of sampling. For each replicate, we calculate the number of haplotypes and the number of segregating sites, which represent our observed values in the simulated samples. The sampling size, that it is known to us from this design, is what we are going to try to predict using the back calculations described above. All the analysis of the simulated populations was done in R [@RCoreTeam2013]. Detailed information on simulations and R code is presented in the supplementary material (for now in file "runningSimulations".




5-3-15 we are now pretty sure that the S=k method is too insensitive (as often the case iwth coalescent, and we see that above samples of 10 the values are relatively insensitive, see Vince Buffalo's "dancing genealogies" for illustration of why even if peaks look sharper for theta 10 the possibilities are massive for the empirical stuff that fits....). So this is concerning for even being able to tell orders of magnitude apart, e.g. 10 of species A and 100 of species B probably won't happen. 

So new structure of paper we think: show that haplotypes are the defined minimum number of individuals, this is the dumb but easy and factual way. Then the gamma distribution, which is a shot in the dark, makes sense but parameterization is hard to think about. Then the 'smart' coalescent way, but we figure out it is not very sensitive.

And then part B of Results would be that we show empirical values of #haplotypes for samples of n given theta, identify whether this helps us parameterize the slightly-smarter approach. We may also ponder the mathematical relationship between theta and hapdiv, but of course for a given theta (truth) there is a range of theta (estimated from empirical) and a range of hapdiv (estimated from empirical)



# Results 

The general information of the simulated populations is presented in Table 1. As expected, the haplotype diversity, number of haplotypes and number of segregating sites are higher in the two populations with theta=10. The the populations that included a growth parameter achieved a negative Tajima's D, that reflects better the natural population in many invertebrates (Wares cite, *John can we generalize what happen to marine invertebrates, or this phenomenon has also been observed in other terrestrial invertebrates?*)

```{r Table 1 with info on simulated populations, results='asis',echo=FALSE}

#install.packages("gap")
#install.packages("knitr")

library(gap)
library(PopGenome)
library(knitr)

# reading the simulated populations files with PopGenome----
readMS("theta2Growth.out")->popgen.t2g #gives a "genome" object
readMS("theta2NoGrowth.out")->popgen.t2ng
readMS("theta10Growth.out")->popgen.t10g
readMS("theta10NoGrowth.out")->popgen.t10ng
readMS("theta20Growth.out")->popgen.t20g
readMS("theta20NoGrowth.out")->popgen.t20ng

# number of segregating sites----
varsites.t2g<-popgen.t2g@n.biallelic.sites
varsites.t2ng<-popgen.t2ng@n.biallelic.sites
varsites.t10g<-popgen.t10g@n.biallelic.sites
varsites.t10ng<-popgen.t10ng@n.biallelic.sites
varsites.t20g<-popgen.t20g@n.biallelic.sites
varsites.t20ng<-popgen.t20ng@n.biallelic.sites
varsites<-c(varsites.t2g,varsites.t2ng,varsites.t10g,varsites.t10ng,varsites.t20g,varsites.t20ng)

# run F_ST stats and check haplotype diversity
F_ST.stats(popgen.t2g)->popgen.t2g
F_ST.stats(popgen.t2ng)->popgen.t2ng
F_ST.stats(popgen.t10g)->popgen.t10g
F_ST.stats(popgen.t10ng)->popgen.t10ng
F_ST.stats(popgen.t20g)->popgen.t20g
F_ST.stats(popgen.t20ng)->popgen.t20ng

# get haplotype diversity for each source population----
unlist(popgen.t2g@region.stats@haplotype.diversity)->hapDiv.t2g
unlist(popgen.t2ng@region.stats@haplotype.diversity)->hapDiv.t2ng
unlist(popgen.t10g@region.stats@haplotype.diversity)->hapDiv.t10g
unlist(popgen.t10ng@region.stats@haplotype.diversity)->hapDiv.t10ng
unlist(popgen.t20g@region.stats@haplotype.diversity)->hapDiv.t20g
unlist(popgen.t20ng@region.stats@haplotype.diversity)->hapDiv.t20ng
hapDiv<-c(hapDiv.t2g,hapDiv.t2ng,hapDiv.t10g,hapDiv.t10ng,hapDiv.t20g,hapDiv.t20ng)

# get haplotype counts----
length(unlist(popgen.t2g@region.stats@haplotype.counts)/1000)->hapCount.t2g
length(unlist(popgen.t2ng@region.stats@haplotype.counts)/1000)->hapCount.t2ng
length(unlist(popgen.t10g@region.stats@haplotype.counts)/1000)->hapCount.t10g
length(unlist(popgen.t10ng@region.stats@haplotype.counts)/1000)->hapCount.t10ng
length(unlist(popgen.t20g@region.stats@haplotype.counts)/1000)->hapCount.t20g
length(unlist(popgen.t20ng@region.stats@haplotype.counts)/1000)->hapCount.t20ng
hapcounts<-c(hapCount.t2g,hapCount.t2ng,hapCount.t10g,hapCount.t10ng,hapCount.t20g,hapCount.t20ng)

# run neutrality.stats to check that Tajima's D is negative and looks real----
neutrality.stats(popgen.t2g)->popgen.t2g
neutrality.stats(popgen.t2ng)->popgen.t2ng
neutrality.stats(popgen.t10g)->popgen.t10g
neutrality.stats(popgen.t10ng)->popgen.t10ng
neutrality.stats(popgen.t20g)->popgen.t20g
neutrality.stats(popgen.t20ng)->popgen.t20ng

Taj.t2g<-popgen.t10g@Tajima.D
Taj.t2ng<-popgen.t10ng@Tajima.D
Taj.t10g<-popgen.t10g@Tajima.D
Taj.t10ng<-popgen.t10ng@Tajima.D
Taj.t20g<-popgen.t20g@Tajima.D
Taj.t20ng<-popgen.t20ng@Tajima.D
tajima<-c(Taj.t2g,Taj.t2ng,Taj.t10g,Taj.t10ng,Taj.t20g,Taj.t20ng)

# summary table
thetas<-c(2,2,10,10,20,20)
pops<-c("Population 1", "Population 2", "Population 3", "Population 4","Population 5","Population 6")
growth<-c("yes","no","yes","no","yes","no")
newhap<-round(hapDiv,2)
newtaj<-round(tajima,2)
sumTable<-cbind(pops,thetas,growth,newhap,newtaj,hapcounts,varsites)
colnames(sumTable)<-c("Population","Theta","Growth","Haplotype diversity","Tajima's D","Number of haplotypes","Number of segregating sites")
kable(sumTable)
```

Using information from the samples of the 4 simulated populations, We first corroborated that the observed number of haplotypes sets the minimum number of individuals present (for a haploid mitochondrial marker). We observed that the sample size is always equal or larger than the minimum number of haplotypes (Figure X1, solid line represents x=y, colors represent the different populations).

```{r figure X1,echo=FALSE,message=FALSE}
#install.packages("plyr")
library(plyr)

# load data with observed haplotypes in the samples
hapdata<-read.csv("numberHaplotypes.csv",header=T)

hapMin<-ddply(hapdata, c("samplingSize","Pop"), summarise, MinHap = min(n.haplotypes,na.rm=T))
data<-hapMin[order(hapMin$Pop),]

palette(rainbow(6)) 
plot(data$MinHap,data$samplingSize,col=data$Pop,pch=19,xlab="Minimum number of haplotypes observed",ylab="Sample size (n)")
abline(0,1)
legend("bottomright",levels(as.factor(data$Pop)),fill=c("red","yellow","green","cyan","blue","magenta"),bg="white",horiz=T)
```

## Gamma approach

We found that the gamma approach better approximates the simulated sampling sizes in low sampling sizes (Figure X2a, lower than 16-32 depending the population) and when theta is larger (theta=10, Figure X2b). The effect of growth was different in the population with different thetas: in theta=2 the population with growth does a little bit better, in tetha=10 the population without growth approximates more close the observed sampling sizes (Figure X2b). But overall, the gamma method really underestimates the simulated n for larger sample sizes (Figure X2a). The probability distributions for the four populations and the 7 sampling sizes considered (2,4,8,16,32,64 and 128) are presented in the Supplementary material.

```{r Figure X2 gamma distribution plots}

#install.packages("ggplot2")
library(ggplot2)

# now we have the dataframe "gammaData" with all the values we need to plot
gammaData<-read.csv("gammaData.csv",header=T)

# subsetting small sampling sizes to see better what happen there
minigamma<-gammaData[gammaData$samplingSize<33,]

# making factors for the ggplot2 pltos
factor(gammaData$Pop)->gammaData$pops
factor(minigamma$Pop)->minigamma$minipops
factor(minigamma$samplingSize)->minigamma$samplingSizeF

# plotting with ggplot, need to fix legends
#http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/
# Fig X2a
ggplot(gammaData, aes(x = samplingSize,y=Max.Gamma.value,fill=pops,color=pops)) + geom_point()+stat_smooth(method="loess")+geom_abline()+scale_fill_discrete(name="Populations",breaks=c("Pop1","Pop2","Pop3","Pop4"),labels=c("Theta=2g", "Theta=2ng","Theta=10g", "Theta=10ng"))

# Fig X2b
# thinking how to plot the subset of samples sizes where gamma works a little bit better....
ggplot(minigamma, aes(x = samplingSizeF,y=Max.Gamma.value,fill=minipops,color=minipops)) + geom_boxplot(aes(fill=minipops))+ylim(0,20)
```

Paula's note: something weird is happening here when we defined shape=H and scale=number of haplotypes, the distribution of expected nubers using the gamma es identical to the empirical distribution of number of haplotypes in the simulated samples. Is the "happrob" object representing the "expected n"? the name is confusing.

## Theta and segregating sites approach

The implementation of Wakeley's [-@Wakeley08] formula crushed for n larger than 50, so we were only able to test our sampling sizes of 2,4,8,16 and 32 with this method.

```{r plotting Wakeley data}

#install.packages("vioplot")

library(lattice)
library(plyr)
library(vioplot)

# Load Wakeley data and observed segregating sites
obsdata<-read.csv("numberSegSites.csv",header=T)
wakdata<-read.csv("WakeleyData.csv",header=T)

# Summarize the info on observed segregating sites
infoObs<-ddply(obsdata, c("Pop","samplingSize"), summarise, medianOb = median(n.seg.sites,na.rm=T),meanOb = mean(n.seg.sites,na.rm=T),quant.25=quantile(n.seg.sites,0.25),quant.75=quantile(n.seg.sites,0.75))

# keep only the n<=32
miniObs<-infoObs[which(infoObs$samplingSize<33),]

# Summarize Wakeley's info 
miniwak<-wakdata[which(wakdata$n<33),]

#----make loop to find the best n's-----
thetas<-c("two","ten","twenty")
a<-c(rep("two",135),rep("ten",135),rep("twenty",135))
b<-c(seq(1,135,1),seq(1,135,1),seq(1,135,1))
goodwak<-as.data.frame(cbind(a,b))
names(goodwak)<-c("theta","seg.sites")

for(i in thetas){
  for(j in 1:nrow(goodwak)){ #Paula this wasn't working should 'mydata' be 'goodwak'??
    goodwak$seg.sites[j]->mysegsite
    mysubset<-subset(miniwak,miniwak$Theta==i & miniwak$segSites==mysegsite)
    max(mysubset$prob)->maxprob
    which(mysubset$prob==maxprob)->best
    mysubset$n[best]->best.n
    best.n->goodwak$best.n[j]
    }
}
# now "goodwak" has theta,seg.sites and best.n

# Now we want to merge the observed segregating sites with the segregating sites used to estimate Wakeley's formula, and that will allow us to plot the "real" n vs the best.n according Wakeley.
bymedian<-merge(miniObs,goodwak,by.x="medianOb",by.y="seg.sites",all.x=T)
bymedian[complete.cases(bymedian$best.n),]->bymedian1
bymedian1$Pop<-factor(bymedian1$Pop)
xyplot(best.n~samplingSize|Pop,data=bymedian1)

 
```


John's part below:

```{r nextfig, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}
#  print(obsvd) #now would be easy to print 95% HPD of obsvd, so for given theta and k this is the distribution of n
  plot(obsvd,xlab="n",ylab="P(n|k)",ylim=c(0,0.15),main=paste("for K =",obsvdk, "and theta =",Q,"in black; for hapdiv in red"))
points(probs[,1],probs[,3],col='red')
abline(v=actual)


```

Remember that the input data for this single taxon included `r actual` individuals, the vertical line in plot above. What is likelihood function? Product of the two distributions? That is too stark in areas where they don't really overlap probabilities. Shouldn't be ZERO there?

Need to then adapt this to a sample from 4-5 species for which there is known information? Or do simulated data (X species, vector of theta in simulation and hapdiv when all said and done, they are after all, related) and adjust the evenness in series of plots to see if evenness/richness gets recovered appropriately, given confidence intervals after all...

n.b. Marc Feldman was concerned about the two statistics double-dipping on the same theory... is this ABC, is this borrowing strength, is this inappropriate?? **AH. but certainly *S* and *H* are not independent observations thus cannot simply use product of the two to generate a Likelihood** perhaps identify which is better, useful, or are neither sufficient?


# Discussion

Returning to the coin flip, it is worth evaluating wherein lies the strength of inferential signal. 50% gives NO information, could be 2 or infinite flips. So it is deviation that is signal in the coin flip example. Similarly, for a system of diversity such as this we need the *potential* for diverse outcomes to evaluate: low theta means nearly all sample sizes are possible, for example. In this sense, developing this with a mind for species that are broadly distributed and highly abundant is likely a more effective strategy than endemic small populations.

It should be noted that the problem we face - unknown input to the diversity observed - is a similar problem that biologists have handled studying species introductions (Wares et al. 2005) exacerbated by the confounding issues of next-generation sequencing.

Talk a bit about how barcode frequency information maybe isn't as far off when dealing with closely related taxa? We aren't throuwing out frequency, goal here is to look solely at complementary information.

In the end, Discussion :: though there are concerns about read frequency...at a minimum haplotpye number bounds the minimum. In this way we feel better about order of magnitude results, and less need for prior information from a population. This has been less evaluated in microbial/viral samples as "population" is perhaps less defined in those communi9ties than in eukaryote or metazoan communiteis, e.g. through gene flow. But as a complementary recognition that the number of haplotypes tells us some information for sure, and prior information about that population may also provide additional information, we may start to improve on our ability to recover actual ecology from actual molecules.

5-3-15 we are now pretty sure that the S=k method is too insensitive (as often the case iwth coalescent, and we see that above samples of 10 the values are relatively insensitive, see Vince Buffalo's "dancing genealogies" for illustration of why even if peaks look sharper for theta 10 the possibilities are massive for the empirical stuff that fits....). So this is concerning for even being able to tell orders of magnitude apart, e.g. 10 of species A and 100 of species B probably won't happen. 


An important note is that we need to be confident that our data from the field population (the haplotype diversity or theta) is representing a "true" population, and it is not including different populations 


another thing to consider with prior knowledge: presumably different species amplify with different efficiency, but WITHIN a population - well, perhaps you can evaluate (a) the frequency of haplotypes, and thus (b) the site frequency spectrum - in other words, you could calculate haplotype diversity as well and see how well it matches prior estimates, and also you could then assume that things like Taj D could be calculated? Could that perform any better at getting you back to actual-n? Probably not.


# Acknowledgments

Idea brought about by extended problem-solving session with J. Drake, helped greatly by C. Ewers-Saucedo and K. Bockrath. Work supported by funding from NSF-OCE-Chile, OVPR, and UGA Department of Genetics.

# Figure captions



# Literature Cited


