---
title: "Explorations, analysis, popurri of stuff"
author: "Paula Pappalardo"
date: "Wednesday, May 13, 2015"
output: pdf_document
---

# Using the gamma approach

Now we have 100 replicates of each sample size, for each population, and we want to run the gamma stats for each replicate in each sampling size. The number of haplotypes are in the file "numberHaplotypes.csv", so we are going to use that file, add the haplotype diversity for each population, and with that calculate the gamma distribution for each row. For now, we are keeping the max.predicted value as to plot against our known sample size.

```{r getting gamma distribution data,eval=F}

# Number of samples, from a log2 distribution, it is the same for all populations
numsamp<-c(2,4,8,16,32,64,128) 

# Haplotype diversity for each of the four simulated populations of 1000 individuals
hapdiv<-c(0.5504204, 0.3377538, 0.8915536, 0.9258679,0.7627047, 0.9565626)

# The number of haplotypes for each replicate is now in a dataframe
hapnum<-read.csv("numberHaplotypes.csv",header=T)
hapnum$X<-NULL

# add the column of haplotype diversity for each population
for(i in 1:6){
hapnum$hap.diversity[hapnum$Pop==i]<-hapdiv[i] 
}

# Loop to fill the gamma expectation in the hapnum dataframe----
for(i in 1:nrow(hapnum)){
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,scale=hapnum$hap.diversity[i],shape=hapnum$n.haplotypes[i]) 
      indprob<-cdf-cdfprev
      happrob<-hapnum$n.haplotypes[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
  probs<-t(matrix(array,nrow=3))
  max<-max(probs[,3])
  maxPred<-probs[which(probs[,3]==max),1]
  hapnum$Max.Gamma.value[i]<-maxPred
  }

# save result so we don't need to rerun----
hapnum$X<-NULL
#write.csv(hapnum,"gammaData.csv")
```

All the gamma data was saved in the "gammaData.csv" file, and now we can directly call that file from our QuBar ms file to do the plots.

# Using theta and segregating sites

There are specific probability distributions associated with a sample of sequences, the number of segregating sites *S*, and a prior assumption of $\theta$ [@Wakeley08].

Below we calculated the distribution of sampling size according to Wakeley's formula, for each observed theta in the natural (simulated) population of the species and for 1 to max number of segregating sites. We only estimated n from 1 to 50 because larger than that it behaves funny. We have 3 loops, one for each theta and then we merge all in the final datasets.

```{r theta and segregating sites, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}
library(reshape)

# load data with observed segregating sites in the samples
data<-read.csv("numberSegSites.csv",header=T)

# we need to define a maximum n, John identified that above 70 this behaves funny, actually for 60 too, so I put 50
maxn=33
maxk<-max(data$n.seg.sites,na.rm=T)
#maxk=100


# -----theta=2-----------
Q = 2 #theta 
# creating empty list to put the results
theta2<-list()

a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b

# looping from 1 to maxk and running Wakeley formula
for (k in 0:maxk){
  name<-paste("seg",k,sep="")
  obsvdk<-k
  for (n in 2:maxn) {
        PSK=0
        for (i in 2:n) {
             PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
        }
        PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
      obsvd<-PSKv[obsvdk,]
      obsvd->theta2[[name]]
      }
    }

# check that all worked out fine
#hist3D(x=seq(0,1,length.out=nrow(PSKv)),y=seq(0,1,length.out=ncol(PSKv)),PSKv,col=NULL,border="black",theta=2,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35)

# The result of the loop is a list "theta2", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.

# put data in a dataframe to handle it more easy
theta.2 <-t(data.frame(do.call(rbind, theta2))) 
as.data.frame(theta.2)->data2
data2$Theta<-"two"
data2$n<-seq(1,maxn,1)
row.names(data2)<-NULL

# -----theta=10-----------
Q = 10 #theta 
# creating empty list to put the results
theta10<-list()
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b

# looping in segdata and running Wakeley formula
for (k in 0:maxk){
  name<-paste("seg",k,sep="")
  obsvdk<-k
  for (n in 2:maxn) {
        PSK=0
        for (i in 2:n) {
             PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
        }
        PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
      obsvd<-PSKv[obsvdk,]
      obsvd->theta10[[name]]
      }
    }
# check that all worked out fine
#hist3D(x=seq(0,1,length.out=nrow(PSKv)),y=seq(0,1,length.out=ncol(PSKv)),PSKv,col=NULL,border="black",theta=2,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35)
# The result of the loop is a list "theta10", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.

# put data in a dataframe to handle it more easy
theta.10 <-t(data.frame(do.call(rbind, theta10)))  
as.data.frame(theta.10)->data10
data10$Theta<-"ten"
data10$n<-seq(1,maxn,1)
row.names(data10)<-NULL

# -----theta=20-----------
Q = 20 #theta 
# creating empty list to put the results
theta20<-list()

  a <- c(1:maxn)
  b <- c(1:maxk)
  PSKv <- numeric(maxn*maxk)
  PSKv <- matrix(PSKv,ncol=maxn)
  colnames(PSKv)<-a
  rownames(PSKv)<-b

# looping in segdata and running Wakeley formula
for (k in 0:maxk){
  name<-paste("seg",k,sep="")
  obsvdk<-k
  
  for (n in 2:maxn) {
        PSK=0
        for (i in 2:n) {
             PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
        }
        PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
      obsvd<-PSKv[obsvdk,]
      obsvd->theta20[[name]]
      }
    }
# check that all worked out fine
#hist3D(x=seq(0,1,length.out=nrow(PSKv)),y=seq(0,1,length.out=ncol(PSKv)),PSKv,col=NULL,border="black",theta=2,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35)
# The result of the loop is a list "theta20", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.

# put data in a dataframe to handle it more easy
theta.20 <-t(data.frame(do.call(rbind, theta20)))  
as.data.frame(theta.20)->data20
data20$Theta<-"twenty"
data20$n<-seq(1,maxn,1)
row.names(data20)<-NULL

# join dataframes and save file----
rbind(data2,data10,data20)->WakeleyData
# reorder the data
all<-melt(WakeleyData,id=c("Theta","n"))
levels(all$variable)<-seq(1:135)
names(all)<-c("Theta","n","segSites","prob")
# save 
write.csv(all,"WakeleyData.csv")
```

As a result, we have a dataframe "WakeleyData" with all the estimates.

Now in our "QuBar_current" we can plot for each sampling size the expected value according Wakeley, and mark the abline with the observed value. And then do that for both theta. 


----------------until here revised------------------------------------

# Using the sampling theory of selectively neutral alleles

For what I read, this uses the number of haplotypes, theta and the sampling n. I think we can use this to backcalculate n.

```{r using sampling theory Ewens 1972}

# load data with observed haplotypesin the samples
data<-read.csv("hapsegData.csv",header=T)
data$segSites<-NULL
data$X<-NULL
factor(data$sampleSize)->data$sampleSize
databp<-data[with(data,order(Populations,sampleSize,Haplotypes)),]
row.names(databp)<-NULL

# subsetting populations
pop1<-databp[which(databp$Populations=='Pop1'),]
pop2<-databp[which(databp$Populations=='Pop2'),]
pop3<-databp[which(databp$Populations=='Pop3'),]
pop4<-databp[which(databp$Populations=='Pop4'),]

# calculate all the different k haplotypes given different values of n
theta=2 #define theta
maxn=128 #define max n

# this loop fills a vector with the expected k for a given theta an differents n
meank<-rep(NA,maxn)
for(n in 2:maxn){
  n-1->endpoint
  res<-rep(NA,endpoint)
  for (i in 1:endpoint){
  theta/(theta+i)->res[i]
  }
  1+sum(res)->meank[n]
}

# this loop fills a vector with the variance of k for a given theta an differents n
vark<-rep(NA,maxn)
for(n in 2:maxn){
  n-1->endpoint
  res<-rep(NA,endpoint)
  for (i in 1:endpoint){
  (theta^2)/((theta+i)^2)->res[i]
  }
  meank[n]-sum(res)->vark[n]
}

# now we put together a dataframe with the n,k,and var(k), run it twice changing theta to make dataframe
meank->meank2; vark->vark2
meank->meank10; vark->vark10
dataSampling<-as.data.frame(cbind(c(1:maxn),meank2,vark2,meank10,vark10))
names(dataSampling)<-c("n","expHap.t2","varHap.t2","expHap.t10","varHap.t10")

# plot variance
with(dataSampling,plot(n,varHap.t10,col="red"))
with(dataSampling,points(n,varHap.t2,col="blue"))
legend("topleft", inset=.05, c("theta=2","theta=10"), fill=c("blue","red"), horiz=TRUE)

# save data file
write.csv(dataSampling,"dataSamplingTheory.csv")
```

# Old code just in case

```{r OLD gamma distribution code,eval=F}
# load libraries
library(lattice)
library(ggplot2)


# Number of samples, from a log2 distribution, it is the same for all populations
numsamp<-c(2,4,8,16,32,64,128) 

# Haplotype diversity for each of the four simulated populations of 1000 individuals
hapdiv<-c(0.5504204, 0.3377538, 0.8915536, 0.9258679)


# Number of haplotypes for each replicates are in a list within a list
# pop1haps (list of 7 sampling sizes, that includes the list with 100 replicates)
# pop2haps
# pop3haps
# pop4haps

# We can make this more efficient, but for now, let's keep a loop for each population:

# set dataframe to fill with the predicted and observed values,for each population, we need 
Population<-rep(NA,700)
Max.Pred.value<-rep(NA,700)
data.frame(Population,Max.Pred.value)->toFill
toFill->toFill2
toFill->toFill3
toFill->toFill4

# Loop in all populations to get the estimates----

# Loop population 1
count<-0
for (j in 1:7){
  pop1haps[[j]]->ourSize
  names(ourSize)<-NULL
    for(i in 1:100){
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,ourSize[i],hapdiv[1]) 
      indprob<-cdf-cdfprev
      happrob<-ourSize[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
  probs<-t(matrix(array,nrow=3))
  probs
  max<-max(probs[,3])
  maxPred<-probs[which(probs[,3]==max),1]
  toFill$Max.Pred.value[i+count]<-maxPred
  toFill$Obs.n[i+count]<-numsamp[j]
  toFill$Population[i+count]<-"Pop1"
  }
  count+100->count
 }

# Loop population 2
count<-0
for (j in 1:7){
  pop2haps[[j]]->ourSize
  names(ourSize)<-NULL
    for(i in 1:100){
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,ourSize[i],hapdiv[1]) 
      indprob<-cdf-cdfprev
      happrob<-ourSize[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
  probs<-t(matrix(array,nrow=3))
  probs
  max<-max(probs[,3])
  maxPred<-probs[which(probs[,3]==max),1]
  toFill2$Max.Pred.value[i+count]<-maxPred
  toFill2$Obs.n[i+count]<-numsamp[j]
  toFill2$Population[i+count]<-"Pop2"
  }
  count+100->count
 }

# Loop population 3
count<-0
for (j in 1:7){
  pop3haps[[j]]->ourSize
  names(ourSize)<-NULL
    for(i in 1:100){
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,ourSize[i],hapdiv[1]) 
      indprob<-cdf-cdfprev
      happrob<-ourSize[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
  probs<-t(matrix(array,nrow=3))
  probs
  max<-max(probs[,3])
  maxPred<-probs[which(probs[,3]==max),1]
  toFill3$Max.Pred.value[i+count]<-maxPred
  toFill3$Obs.n[i+count]<-numsamp[j]
  toFill3$Population[i+count]<-"Pop3"
  }
  count+100->count
 }

# Loop population 4
count<-0
for (j in 1:7){
  pop4haps[[j]]->ourSize
  names(ourSize)<-NULL
    for(i in 1:100){
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,ourSize[i],hapdiv[1]) 
      indprob<-cdf-cdfprev
      happrob<-ourSize[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
  probs<-t(matrix(array,nrow=3))
  probs
  max<-max(probs[,3])
  maxPred<-probs[which(probs[,3]==max),1]
  toFill4$Max.Pred.value[i+count]<-maxPred
  toFill4$Obs.n[i+count]<-numsamp[j]
  toFill4$Population[i+count]<-"Pop4"
  }
  count+100->count
 }

# putting populations together
rbind(toFill,toFill2,toFill3,toFill4)->gammaData

# With this I can do a summary table of the frequency of predicted haplotypes for each observed value
table(toFill$Pred.value,toFill$Obs.n)

# save result so we don't need to rerun----
write.csv(gammaData,"gammaData.csv")
```

```{r gamma summarizing plots}
library(lattice)
library(evmix) #it includes gamma kernels

# load data with observed haplotypesin the samples
data<-read.csv("hapsegData.csv",header=T)
factor(data$sampleSize)->data$sampleSizeF
data<-data[with(data,order(Haplotypes)),]
databp<-data[with(data,order(Populations,sampleSizeF)),]

# plotting density kernels
plot(density(pop3$Haplotypes[401:500],bw=0.89))

# plotting multiples boxplot with lattice
with(data,bwplot(Haplotypes~sampleSizeF|Populations,layout=c(2,2)))

pop1<-databp[which(databp$Populations=='Pop1'),]
pop2<-databp[which(databp$Populations=='Pop2'),]
pop3<-databp[which(databp$Populations=='Pop3'),]
uni3<-unique(pop3$Haplotypes)
pop4<-databp[which(databp$Populations=='Pop4'),]

par(mfrow=c(1,2))
vio2<-pop1$Haplotypes[pop1$sampleSize==2]
vio4<-pop1$Haplotypes[pop1$sampleSize==4]
vio8<-pop1$Haplotypes[pop1$sampleSize==8]
vio16<-pop1$Haplotypes[pop1$sampleSize==16]
vio32<-pop1$Haplotypes[pop1$sampleSize==32]
vio64<-pop1$Haplotypes[pop1$sampleSize==64]
vio128<-pop1$Haplotypes[pop1$sampleSize==128]
vioplot(vio2,vio4,vio8,vio16,vio32,vio64,vio128,col="aquamarine",names=c("n2","n4","n8","n16","n32","n64","n128"),horizontal=T)
title("Population 1-theta 2")

vio2<-pop3$Haplotypes[pop3$sampleSize==2]
vio4<-pop3$Haplotypes[pop3$sampleSize==4]
vio8<-pop3$Haplotypes[pop3$sampleSize==8]
vio16<-pop3$Haplotypes[pop3$sampleSize==16]
vio32<-pop3$Haplotypes[pop3$sampleSize==32]
vio64<-pop3$Haplotypes[pop3$sampleSize==64]
vio128<-pop3$Haplotypes[pop3$sampleSize==128]
vioplot(vio2,vio4,vio8,vio16,vio32,vio64,vio128,col="darkcyan",names=c("n2","n4","n8","n16","n32","n64","n128"),horizontal=T)
title("Population 3-theta 10")


# to plot multiples histogram with lattice
with(pop2,histogram(~Haplotypes|sampleSizeF,layout=c(2,4)))

plot(data$Haplotypes,col=data$sampleSize)
plot(data$Haplotypes,col=data$Population)
```

```{r OLD theta and segregating sites, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}
library(plot3D)

# load data with observed segregating sites in the samples
data<-read.csv("hapsegData.csv",header=T)

# getting the unique combinations of segregating sites per population/theta to enter in the Wakeley formula
segdata<-unique(data[,c("Populations","segSites","sampleSize")])

# taking out the values of zero segregating sites
zero<-which(segdata$segSites==0)
segdata[-zero,]->segdata

# we need to define a maximum n, John identified that above 70 this behaves funny, actually for 60 too, so I put 50
maxn=50
maxk<-max(segdata$segSites)

# theta values in our four populations, our "known" value of the "real" population (n=1000 simulated populations in this round)
theta<-c(2,2,10,10)

# -----theta=2-----------
Q = 2 #theta 
# subsetting data from the populations with theta=2
segdata2<-subset(segdata,Populations=="Pop1"|Populations=="Pop2")

# creating empty list to put the results
ndistT2<-list()

# looping in segdata and running Wakeley formula
for (z in 1:nrow(segdata2)){
  name<-paste("seg",z,sep="")
  obsvdk<-segdata2$segSites[z]
  a <- c(1:maxn)
  b <- c(1:maxk)
  PSKv <- numeric(maxn*maxk)
  PSKv <- matrix(PSKv,ncol=maxn)
  colnames(PSKv)<-a
  rownames(PSKv)<-b
  for (n in 2:maxn) {
        PSK=0
        for (i in 2:n) {
             PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
        }
        PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
      obsvd<-PSKv[obsvdk,]
      obsvd->ndistT2[[name]]
      }
    }
  
# The result of the loop is a list "ndistT2", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.

# -----theta=10-----------
Q = 10 #theta 
# subsetting data from the populations with theta=10
segdata10<-subset(segdata,Populations=="Pop3"|Populations=="Pop4")

# creating empty list to put the results
ndistT10<-list()

# looping in segdata and running Wakeley formula
for (z in 1:nrow(segdata10)){
  name<-paste("seg",z,sep="")
  obsvdk<-segdata2$segSites[z]
  a <- c(1:maxn)
  b <- c(1:maxk)
  PSKv <- numeric(maxn*maxk)
  PSKv <- matrix(PSKv,ncol=maxn)
  colnames(PSKv)<-a
  rownames(PSKv)<-b
  for (n in 2:maxn) {
        PSK=0
        for (i in 2:n) {
             PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
        }
        PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
      obsvd<-PSKv[obsvdk,]
      obsvd->ndistT10[[name]]
      }
    }
  
# The result of the loop is a list "ndistT10", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.


#hist3D(x=seq(0,1,length.out=nrow(PSKv)),y=seq(0,1,length.out=ncol(PSKv)),PSKv,col=NULL,border="black",theta=2,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35,main=paste((expression(theta))," = ",Q))

#  print(obsvd) #now would be easy to print 95% HPD of obsvd, so for given theta and k this is the distribution of n
# plot(obsvd,xlab="n",ylab="P(n|k)",ylim=c(0,0.15),main=paste("for K =",obsvdk, "and theta =",Q,"in black; for hapdiv in red"))
```
