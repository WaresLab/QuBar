---
title: "Explorations, analysis, popurri of stuff"
author: "Paula Pappalardo"
date: "Wednesday, May 13, 2015"
output: pdf_document
---

# Using the gamma approach

Now we have 100 replicates of each sample size, for each population, and we want to run the gamma stats for each replicate in each sampling size. The number of haplotypes are in the file "numberHaplotypes.csv", so we are going to use that file, add the haplotype diversity for each population, and with that calculate the gamma distribution for each row. For now, we are keeping the max.predicted value as to plot against our known sample size.

```{r getting gamma distribution data,eval=F}

# Number of samples, from a log2 distribution, it is the same for all populations
numsamp<-c(2,4,8,16,32,64,128) 

# Haplotype diversity for each of the four simulated populations of 1000 individuals
hapdiv<-c(0.5504204, 0.3377538, 0.8915536, 0.9258679,0.7627047, 0.9565626)

# The number of haplotypes for each replicate is now in a dataframe
hapnum<-read.csv("numberHaplotypes.csv",header=T)
hapnum$X<-NULL

# add the column of haplotype diversity for each population
for(i in 1:6){
hapnum$hap.diversity[hapnum$Pop==i]<-hapdiv[i] 
}

# Loop to fill the gamma expectation in the hapnum dataframe----
for(i in 1:nrow(hapnum)){
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,shape=(1/hapnum$hap.diversity[i]),rate=(1/hapnum$n.haplotypes[i])) 
      indprob<-cdf-cdfprev
      happrob<-hapnum$n.haplotypes[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
  probs<-t(matrix(array,nrow=3))
  max<-max(probs[,3])
  maxPred<-probs[which(probs[,3]==max),1]
  hapnum$Max.Gamma.value[i]<-maxPred
  }

# save result so we don't need to rerun----
write.csv(hapnum,"gammaData_13Jul.csv")
```

All the gamma data was saved in the "gammaData.csv" file, and now we can directly call that file from our QuBar ms file to do the plots.

# Using theta and segregating sites (Wakeley)

There are specific probability distributions associated with a sample of sequences, the number of segregating sites *S*, and a prior assumption of $\theta$ [@Wakeley08].

Below we calculated the distribution of sampling size according to Wakeley's formula, for each observed theta in the natural (simulated) population of the species and for 1 to max number of segregating sites. We only estimated n from 1 to 50 because larger than that it behaves funny. We have 3 loops, one for each theta and then we merge all in the final datasets.

```{r Wakeley: theta and segregating sites, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}
library(reshape)
library(plot3D)

# load data with observed segregating sites in the samples
data<-read.csv("numberSegSites.csv",header=T)

# We need to define a maximum n, John identified that above 70 this behaves funny, actually for 60 too, so we could 50. But as our simulated sampling sizes jump from 32 to 64, we can just set a maxn of 33.

maxn=33
maxk<-max(data$n.seg.sites,na.rm=T)
#maxk=100

# -----theta=2-----------
Q = 2 #theta 
# creating empty list to put the results
theta2<-list()

a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b

# looping from 1 to maxk and running Wakeley formula
for (k in 0:maxk){
  name<-paste("seg",k,sep="")
  obsvdk<-k
  for (n in 2:maxn) {
        PSK=0
        for (i in 2:n) {
             PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
        }
        PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
      obsvd<-PSKv[obsvdk,]
      obsvd->theta2[[name]]
      }
    }

# check that all worked out fine
hist3D(x=seq(0,1,length.out=nrow(PSKv)),y=seq(0,1,length.out=ncol(PSKv)),PSKv,col=NULL,border="black",theta=2,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35)

# The result of the loop is a list "theta2", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.

# put data in a dataframe to handle it more easy
theta.2 <-t(data.frame(do.call(rbind, theta2))) 
as.data.frame(theta.2)->data2
data2$Theta<-"two"
data2$n<-seq(1,maxn,1)
row.names(data2)<-NULL

# -----theta=10-----------
Q = 10 #theta 
# creating empty list to put the results
theta10<-list()
a <- c(1:maxn)
b <- c(1:maxk)
PSKv <- numeric(maxn*maxk)
PSKv <- matrix(PSKv,ncol=maxn)
colnames(PSKv)<-a
rownames(PSKv)<-b

# looping in segdata and running Wakeley formula
for (k in 0:maxk){
  name<-paste("seg",k,sep="")
  obsvdk<-k
  for (n in 2:maxn) {
        PSK=0
        for (i in 2:n) {
             PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
        }
        PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
      obsvd<-PSKv[obsvdk,]
      obsvd->theta10[[name]]
      }
    }
# check that all worked out fine
#hist3D(x=seq(0,1,length.out=nrow(PSKv)),y=seq(0,1,length.out=ncol(PSKv)),PSKv,col=NULL,border="black",theta=2,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35)
# The result of the loop is a list "theta10", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.

# put data in a dataframe to handle it more easy
theta.10 <-t(data.frame(do.call(rbind, theta10)))  
as.data.frame(theta.10)->data10
data10$Theta<-"ten"
data10$n<-seq(1,maxn,1)
row.names(data10)<-NULL

# -----theta=20-----------
Q = 20 #theta 
# creating empty list to put the results
theta20<-list()

  a <- c(1:maxn)
  b <- c(1:maxk)
  PSKv <- numeric(maxn*maxk)
  PSKv <- matrix(PSKv,ncol=maxn)
  colnames(PSKv)<-a
  rownames(PSKv)<-b

# looping in segdata and running Wakeley formula
for (k in 0:maxk){
  name<-paste("seg",k,sep="")
  obsvdk<-k
  
  for (n in 2:maxn) {
        PSK=0
        for (i in 2:n) {
             PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
        }
        PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
      obsvd<-PSKv[obsvdk,]
      obsvd->theta20[[name]]
      }
    }
# check that all worked out fine
#hist3D(x=seq(0,1,length.out=nrow(PSKv)),y=seq(0,1,length.out=ncol(PSKv)),PSKv,col=NULL,border="black",theta=2,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35)
# The result of the loop is a list "theta20", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.

# put data in a dataframe to handle it more easy
theta.20 <-t(data.frame(do.call(rbind, theta20)))  
as.data.frame(theta.20)->data20
data20$Theta<-"twenty"
data20$n<-seq(1,maxn,1)
row.names(data20)<-NULL

# join dataframes and save file----
rbind(data2,data10,data20)->WakeleyData
# reorder the data
all<-melt(WakeleyData,id=c("Theta","n"))
levels(all$variable)<-seq(1:135)
names(all)<-c("Theta","n","segSites","prob")
# save 
write.csv(all,"WakeleyData.csv")
```

As a result, we have a dataframe "WakeleyData" with all the estimates.


# Using the sampling theory of selectively neutral alleles

For what I read, this uses the number of haplotypes, theta and the sampling n. I think we can use this to backcalculate n.

```{r using sampling theory Ewens 1972}
# calculate all the different k haplotypes given different values of n, changes thetas and save the corresponding vectors

maxn = 250 #define max n to get enough haplotypes to match our pops

# ----Theta 2-----
theta = 2

# this loop fills a vector with the expected k for a given theta and differents n
meank2 <- rep(NA,maxn) #creates vector to save results

for(n in 1:maxn){
    if(n==1){meank2[n]<-1}
      else{
      n-1->endpoint
      res<-rep(NA,endpoint)
      for (i in 1:endpoint){
      theta/(theta+i)->res[i]
      }
    1+sum(res)->meank2[n]
    }
  }

# this loop fills a vector with the variance of k for a given theta an differents n
vark2 <- rep(NA,maxn)
for(n in 2:maxn){
    n-1->endpoint
    res<-rep(NA,endpoint)
    for (i in 1:endpoint){
    (theta^2)/((theta+i)^2)->res[i]
    }
  meank2[n]-sum(res)->vark2[n]
  1 -> vark2[1]
}

# ----Theta 10-----
theta=10

meank10 <- rep(NA,maxn)
for(n in 1:maxn){
    if(n==1){meank10[n]<-1}
    else{
    n-1->endpoint
    res<-rep(NA,endpoint)
    for (i in 1:endpoint){
    theta/(theta+i)->res[i]
    }
  1+sum(res)->meank10[n]
  }
}
  
vark10 <- rep(NA,maxn)
for(n in 2:maxn){
    n-1->endpoint
    res<-rep(NA,endpoint)
    for (i in 1:endpoint){
    (theta^2)/((theta+i)^2)->res[i]
    }
  meank10[n]-sum(res)->vark10[n]
  1 -> vark10[1]
}

# ----Theta 20-----
theta=20

meank20 <- rep(NA,maxn)
for(n in 2:maxn){
  if(n==1){meank20[n]<-1}
      else{
    n-1->endpoint
    res<-rep(NA,endpoint)
    for (i in 1:endpoint){
    theta/(theta+i)->res[i]
    }
  1+sum(res)->meank20[n]
  }
}

vark20 <- rep(NA,maxn)
for(n in 2:maxn){
    n-1->endpoint
    res<-rep(NA,endpoint)
    for (i in 1:endpoint){
    (theta^2)/((theta+i)^2)->res[i]
    }
  meank20[n]-sum(res)->vark20[n]
  1 -> vark20[1]
}

# now we put together a dataframe with the n,k,and var(k)---- 
theta.df <- c(rep("two",250),rep("ten",250),rep("twenty",250))
n.df <- rep(1:maxn,3) 
exp.haplotypes <- c(meank2,meank10,meank20) 
exp.variance <- c(vark2,vark10,vark20)
dataSampling <- as.data.frame(cbind(theta.df,n.df,exp.haplotypes,exp.variance))

# save data file
write.csv(dataSampling,"dataSamplingTheory_21Ago2015.csv")
```
Now we have the dataset "dataSamplingTheory_23Jul2015" and we can plot the expected number of haplotypes for each theta (and the variance).

```{r check with Ewens table}
dataSampling[which(dataSampling$n.df==250),]
dataSampling[which(dataSampling$n.df==100),]
dataSampling[which(dataSampling$n.df==10),]
```

```{r plots sampling theory first try}

# One type of plots
# plot expected number of haplotypes
with(dataSampling,plot(n,expHap.t20,col="red",main="Expected number of haplotypes",pch=19,ylim=c(1,50),ylab="Expected n° haplotypes",xlab="Sampling size"))
with(dataSampling,points(n,expHap.t10,col="blue",pch=19))
with(dataSampling,points(n,expHap.t2,col="black",pch=19))
legend("topleft", inset=.05, c("theta=20","theta=10","theta=2"), fill=c("red","blue","black"), horiz=TRUE)

# plot variance
with(dataSampling,plot(n,varHap.t20,col="red",main="Variance in the number of haplotypes"))
with(dataSampling,points(n,varHap.t10,col="blue"))
with(dataSampling,points(n,varHap.t2,col="black"))
legend("topleft", inset=.05, c("theta=20","theta=10","theta=2"), fill=c("red","blue","black"), horiz=TRUE)

```

Now we are going to invert the plots so we have the observed number of haplotypes on the x-axis, and we are going to plot the simulated data with sampling theory, plus the standard error to mark the confidence intervals. On top of that, we add the points of the "observed" data.

```{r compare with simulated data}
# load data with observed haplotypes in the samples
data<-read.csv("numberHaplotypes.csv",header=T)
theory<-read.csv("dataSamplingTheory_23Jul2015.csv",header=T)

# prepare observed data and subset populations
databp<-data[with(data,order(Pop,samplingSize)),]
row.names(databp)<-NULL
databp$Pop.f<-as.factor(databp$Pop)
databp$n<-as.factor(databp$samplingSize)

# no growth
two<-databp[which(databp$Pop==2),]
ten<-databp[which(databp$Pop==4),]
twenty<-databp[which(databp$Pop==6),]

# growth
two.g<-databp[which(databp$Pop==1),]
ten.g<-databp[which(databp$Pop==3),]
twenty.g<-databp[which(databp$Pop==5),]

# calculate standard error in theory data
theory$se.t2<-sqrt(theory$varHap.t2)
theory$se.t10<-sqrt(theory$varHap.t10)
theory$se.t20<-sqrt(theory$varHap.t20)

# plot expected number of haplotypes + observed in simulations
with(theory,plot(expHap.t20,n,col="red",main="Expected number of haplotypes",pch=19,ylim=c(1,130),xlim=c(1,50),xlab="Ex. n° haplotypes-no growth",ylab="Sampling size",type="l",lwd=2))
with(theory,points(expHap.t10,n,col="blue",pch=19,type="l",lwd=2))
with(theory,points(expHap.t2,n,col="black",pch=19,type="l",lwd=2))

# ------add standard error

# theta 2
points(theory$expHap.t2-(2*theory$se.t2),theory$n,col="black",pch=19,type="l",lwd=1)
points(theory$expHap.t2+(2*theory$se.t2),theory$n,col="black",pch=19,type="l",lwd=1)

# theta 10
points(theory$expHap.t10-(2*theory$se.t10),theory$n,col="blue",pch=19,type="l",lwd=1)
points(theory$expHap.t10+(2*theory$se.t10),theory$n,col="blue",pch=19,type="l",lwd=1)

# theta 20
points(theory$expHap.t20-(2*theory$se.t20),theory$n,col="red",pch=19,type="l",lwd=1)
points(theory$expHap.t20+(2*theory$se.t20),theory$n,col="red",pch=19,type="l",lwd=1)

legend("bottomright", inset=.01, cex=0.7,c("theta=20","theta=10","theta=2"), fill=c("red","blue","black"), horiz=TRUE)


# ------add points of populations with NO growth
points(twenty$n.haplotypes,twenty$samplingSize-0.5,col="red",cex=0.5,pch=19)
points(ten$n.haplotypes,ten$samplingSize,col="blue",cex=0.5,pch=19)
points(two$n.haplotypes,two$samplingSize+0.5,col="black",cex=0.5,pch=19)
```
Maybe useful? ...other code
```{r}
# add points of populations with growth
points(twenty.g$samplingSize-0.5,twenty.g$n.haplotypes,col="red",pch=19,cex=0.5)
points(ten.g$samplingSize,ten.g$n.haplotypes,col="blue",pch=19,cex=0.5)
points(two.g$samplingSize+0.5,two.g$n.haplotypes,col="black",pch=19,cex=0.5)


histogram(~n.haplotypes|Pop.f,data=databp,layout=c(2,3))

ddply(databp,"Pop.f",summarise,min=min(n.haplotypes,na.rm=T),max=max(n.haplotypes,na.rm=T))
```




----------------until here revised------------------------------------

# Getting theta from pop gen datasets

```{r theta in natural populations}
# load libraries
library(PopGenome) 

# converting FASTA file to GENOME object
popGen<-readData("C:/Users/Paula/Documents/GitHub/QuBar/popGenData",include.unknown=T)
popGen@n.sites # gives the number of sites in the alignment

Varsites<-popGen@n.biallelic.sites #number of biallelic sites (SNPs)

# -------aplying the "F_ST stats" methods
F_ST.stats(popGen)->divResults

# get haplotype diversity for each dataset----
unlist(divResults@region.stats@haplotype.diversity)->results

# get all diversity results
get.diversity(divResults)[[1]]

# --------aplying the "neutrality.stats"
neuResults<-neutrality.stats(popGen)

# get neutrality results
get.neutrality(neuResults,theta=T)[[1]]
```

# Getting theta from Wares 2010 datasets

```{r data from natural populations}
library(plyr)

# load summary table from the excel files of Wares2010
wares<-read.csv("allData_Wares2010.csv",header=T)

# We need to multiply the Waterson's theta obtained by John's sript for the number of sites to get the same theta that we get with PopGenome.
wares$wat.theta<-wares$nsites*wares$ThetaW

# Summarize data by group
waresSum<-ddply(wares,~group,summarise,mean.wat.theta=mean(wat.theta,na.rm=T),median.wat.theta=median(wat.theta,na.rm=T),mean.samples=mean(nsam,na.rm=T),median.samples=median(nsam,na.rm=T),mean.seg.sites=mean(S,na.rm=T),median.seg.sites=median(S,na.rm=T))

```

# Old code just in case

```{r OLD gamma distribution code,eval=F}
# load libraries
library(lattice)
library(ggplot2)


# Number of samples, from a log2 distribution, it is the same for all populations
numsamp<-c(2,4,8,16,32,64,128) 

# Haplotype diversity for each of the four simulated populations of 1000 individuals
hapdiv<-c(0.5504204, 0.3377538, 0.8915536, 0.9258679)


# Number of haplotypes for each replicates are in a list within a list
# pop1haps (list of 7 sampling sizes, that includes the list with 100 replicates)
# pop2haps
# pop3haps
# pop4haps

# We can make this more efficient, but for now, let's keep a loop for each population:

# set dataframe to fill with the predicted and observed values,for each population, we need 
Population<-rep(NA,700)
Max.Pred.value<-rep(NA,700)
data.frame(Population,Max.Pred.value)->toFill
toFill->toFill2
toFill->toFill3
toFill->toFill4

# Loop in all populations to get the estimates----

# Loop population 1
count<-0
for (j in 1:7){
  pop1haps[[j]]->ourSize
  names(ourSize)<-NULL
    for(i in 1:100){
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,ourSize[i],hapdiv[1]) 
      indprob<-cdf-cdfprev
      happrob<-ourSize[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
  probs<-t(matrix(array,nrow=3))
  probs
  max<-max(probs[,3])
  maxPred<-probs[which(probs[,3]==max),1]
  toFill$Max.Pred.value[i+count]<-maxPred
  toFill$Obs.n[i+count]<-numsamp[j]
  toFill$Population[i+count]<-"Pop1"
  }
  count+100->count
 }

# Loop population 2
count<-0
for (j in 1:7){
  pop2haps[[j]]->ourSize
  names(ourSize)<-NULL
    for(i in 1:100){
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,ourSize[i],hapdiv[1]) 
      indprob<-cdf-cdfprev
      happrob<-ourSize[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
  probs<-t(matrix(array,nrow=3))
  probs
  max<-max(probs[,3])
  maxPred<-probs[which(probs[,3]==max),1]
  toFill2$Max.Pred.value[i+count]<-maxPred
  toFill2$Obs.n[i+count]<-numsamp[j]
  toFill2$Population[i+count]<-"Pop2"
  }
  count+100->count
 }

# Loop population 3
count<-0
for (j in 1:7){
  pop3haps[[j]]->ourSize
  names(ourSize)<-NULL
    for(i in 1:100){
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,ourSize[i],hapdiv[1]) 
      indprob<-cdf-cdfprev
      happrob<-ourSize[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
  probs<-t(matrix(array,nrow=3))
  probs
  max<-max(probs[,3])
  maxPred<-probs[which(probs[,3]==max),1]
  toFill3$Max.Pred.value[i+count]<-maxPred
  toFill3$Obs.n[i+count]<-numsamp[j]
  toFill3$Population[i+count]<-"Pop3"
  }
  count+100->count
 }

# Loop population 4
count<-0
for (j in 1:7){
  pop4haps[[j]]->ourSize
  names(ourSize)<-NULL
    for(i in 1:100){
      x=1
      cdf=0
      indprob=0
      array<-NULL
    while (cdf<0.99) {
      cdfprev<-cdf
      cdf<-pgamma(x,ourSize[i],hapdiv[1]) 
      indprob<-cdf-cdfprev
      happrob<-ourSize[i]+(x-1)
      array<-c(array,happrob)
      array<-c(array,cdf)
      array<-c(array,indprob)
      x=x+1
      }
  probs<-t(matrix(array,nrow=3))
  probs
  max<-max(probs[,3])
  maxPred<-probs[which(probs[,3]==max),1]
  toFill4$Max.Pred.value[i+count]<-maxPred
  toFill4$Obs.n[i+count]<-numsamp[j]
  toFill4$Population[i+count]<-"Pop4"
  }
  count+100->count
 }

# putting populations together
rbind(toFill,toFill2,toFill3,toFill4)->gammaData

# With this I can do a summary table of the frequency of predicted haplotypes for each observed value
table(toFill$Pred.value,toFill$Obs.n)

# save result so we don't need to rerun----
write.csv(gammaData,"gammaData.csv")
```

```{r gamma summarizing plots}
library(lattice)
library(evmix) #it includes gamma kernels

# load data with observed haplotypesin the samples
data<-read.csv("hapsegData.csv",header=T)
factor(data$sampleSize)->data$sampleSizeF
data<-data[with(data,order(Haplotypes)),]
databp<-data[with(data,order(Populations,sampleSizeF)),]

# plotting density kernels
plot(density(pop3$Haplotypes[401:500],bw=0.89))

# plotting multiples boxplot with lattice
with(data,bwplot(Haplotypes~sampleSizeF|Populations,layout=c(2,2)))

pop1<-databp[which(databp$Populations=='Pop1'),]
pop2<-databp[which(databp$Populations=='Pop2'),]
pop3<-databp[which(databp$Populations=='Pop3'),]
uni3<-unique(pop3$Haplotypes)
pop4<-databp[which(databp$Populations=='Pop4'),]

par(mfrow=c(1,2))
vio2<-pop1$Haplotypes[pop1$sampleSize==2]
vio4<-pop1$Haplotypes[pop1$sampleSize==4]
vio8<-pop1$Haplotypes[pop1$sampleSize==8]
vio16<-pop1$Haplotypes[pop1$sampleSize==16]
vio32<-pop1$Haplotypes[pop1$sampleSize==32]
vio64<-pop1$Haplotypes[pop1$sampleSize==64]
vio128<-pop1$Haplotypes[pop1$sampleSize==128]
vioplot(vio2,vio4,vio8,vio16,vio32,vio64,vio128,col="aquamarine",names=c("n2","n4","n8","n16","n32","n64","n128"),horizontal=T)
title("Population 1-theta 2")

vio2<-pop3$Haplotypes[pop3$sampleSize==2]
vio4<-pop3$Haplotypes[pop3$sampleSize==4]
vio8<-pop3$Haplotypes[pop3$sampleSize==8]
vio16<-pop3$Haplotypes[pop3$sampleSize==16]
vio32<-pop3$Haplotypes[pop3$sampleSize==32]
vio64<-pop3$Haplotypes[pop3$sampleSize==64]
vio128<-pop3$Haplotypes[pop3$sampleSize==128]
vioplot(vio2,vio4,vio8,vio16,vio32,vio64,vio128,col="darkcyan",names=c("n2","n4","n8","n16","n32","n64","n128"),horizontal=T)
title("Population 3-theta 10")


# to plot multiples histogram with lattice
with(pop2,histogram(~Haplotypes|sampleSizeF,layout=c(2,4)))

plot(data$Haplotypes,col=data$sampleSize)
plot(data$Haplotypes,col=data$Population)
```

```{r OLD theta and segregating sites, echo=FALSE,warning=FALSE,results='hide',message=FALSE,fig.show='asis'}
library(plot3D)

# load data with observed segregating sites in the samples
data<-read.csv("hapsegData.csv",header=T)

# getting the unique combinations of segregating sites per population/theta to enter in the Wakeley formula
segdata<-unique(data[,c("Populations","segSites","sampleSize")])

# taking out the values of zero segregating sites
zero<-which(segdata$segSites==0)
segdata[-zero,]->segdata

# we need to define a maximum n, John identified that above 70 this behaves funny, actually for 60 too, so I put 50
maxn=50
maxk<-max(segdata$segSites)

# theta values in our four populations, our "known" value of the "real" population (n=1000 simulated populations in this round)
theta<-c(2,2,10,10)

# -----theta=2-----------
Q = 2 #theta 
# subsetting data from the populations with theta=2
segdata2<-subset(segdata,Populations=="Pop1"|Populations=="Pop2")

# creating empty list to put the results
ndistT2<-list()

# looping in segdata and running Wakeley formula
for (z in 1:nrow(segdata2)){
  name<-paste("seg",z,sep="")
  obsvdk<-segdata2$segSites[z]
  a <- c(1:maxn)
  b <- c(1:maxk)
  PSKv <- numeric(maxn*maxk)
  PSKv <- matrix(PSKv,ncol=maxn)
  colnames(PSKv)<-a
  rownames(PSKv)<-b
  for (n in 2:maxn) {
        PSK=0
        for (i in 2:n) {
             PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
        }
        PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
      obsvd<-PSKv[obsvdk,]
      obsvd->ndistT2[[name]]
      }
    }
  
# The result of the loop is a list "ndistT2", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.

# -----theta=10-----------
Q = 10 #theta 
# subsetting data from the populations with theta=10
segdata10<-subset(segdata,Populations=="Pop3"|Populations=="Pop4")

# creating empty list to put the results
ndistT10<-list()

# looping in segdata and running Wakeley formula
for (z in 1:nrow(segdata10)){
  name<-paste("seg",z,sep="")
  obsvdk<-segdata2$segSites[z]
  a <- c(1:maxn)
  b <- c(1:maxk)
  PSKv <- numeric(maxn*maxk)
  PSKv <- matrix(PSKv,ncol=maxn)
  colnames(PSKv)<-a
  rownames(PSKv)<-b
  for (n in 2:maxn) {
        PSK=0
        for (i in 2:n) {
             PSK<-PSK + ((-1)^i)*(choose((n-1),(i-1)))*((i-1)/(Q+i-1))*(Q/(Q+i-1))^obsvdk
        }
        PSKv[obsvdk,n]<-PSKv[obsvdk,n]+PSK
      obsvd<-PSKv[obsvdk,]
      obsvd->ndistT10[[name]]
      }
    }
  
# The result of the loop is a list "ndistT10", with each component being one observed segregating site value, and within that a vector with length maxn, and the components are the probability values for that segsite according to Wakeley.


#hist3D(x=seq(0,1,length.out=nrow(PSKv)),y=seq(0,1,length.out=ncol(PSKv)),PSKv,col=NULL,border="black",theta=2,phi=30,xlab="k",ylab="n",zlab="P{S=k}",alpha=0.35,main=paste((expression(theta))," = ",Q))

#  print(obsvd) #now would be easy to print 95% HPD of obsvd, so for given theta and k this is the distribution of n
# plot(obsvd,xlab="n",ylab="P(n|k)",ylim=c(0,0.15),main=paste("for K =",obsvdk, "and theta =",Q,"in black; for hapdiv in red"))
```
