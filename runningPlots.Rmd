---
title: "plots for QuBar project"
author: "Paula Pappalardo"
date: "Wednesday, August 19, 2015"
output: pdf_document
---

# Working on plots...

## Haplotype diversity and gamma estimation

We found that the approach using a Gamma distribution to estimate the number of input individuals does a better job of estimating the simulated sampling sizes in low input sample sizes (Figure 3, lower than 16-32 depending the population) and when $\theta$ is larger ($\theta$=10 or 20, Figure X2b). Overall, using haplotype diversity and an educated guess at how this diversity reflects the input tends to greatly underestimate the simulated sample (Figure 3). 

```{r Figure 3 gamma distribution plots,message=FALSE, echo=FALSE}
setwd("~/GitHub/QuBar")

#install.packages("ggplot2")
library(ggplot2)

# now we have the dataframe "gammaData_13jul" with all the updated gamma values 
fullgammaData<-read.csv("gammaData_13Jul.csv",header=T) 
#JOHN: this one is the updated file after your new twist on gamma, I deleted the other one

# taking out data with growth
gammaData<-subset(fullgammaData,fullgammaData$Growth=="no")
droplevels(gammaData)->gammaData

# making factors for the ggplot2 plots
factor(gammaData$samplingSize,levels=c(2,4,8,16,32,64,128))->gammaData$samplingSizeF

# subsetting small sampling sizes to see better what happen there
#minigamma<-gammaData[gammaData$samplingSize<33,]

# plotting with ggplot, need to fix legends
#http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/
# Fig X2a
#ggplot(gammaData, aes(x = samplingSize,y=Max.Gamma.value,fill=Theta,color=Theta)) + theme_bw()+geom_point()+geom_abline()+xlab("simulated sampling size") + ylab("predicted sampling size") 

# Fig X2b
# thinking how to plot the subset of samples sizes where gamma works a little bit better....
#ggplot(minigamma, aes(x = samplingSizeF,y=Max.Gamma.value,fill=Theta,color=Theta)) + theme_bw()+ geom_boxplot(aes(fill=Theta)) + ylim(0,20) + xlab("simulated sampling size") + ylab("predicted sampling size")

#JOHN, trying something different below
fig3<-ggplot(gammaData, aes(x = n.haplotypes,y=Max.Gamma.value,color=samplingSizeF,size=1)) + theme_bw()+geom_point()+facet_grid(Theta~samplingSizeF) 
fig3 + geom_hline(aes(yintercept = 2), subset(gammaData,samplingSize==2))+ geom_hline(aes(yintercept = 4), subset(gammaData,samplingSize==4))+ geom_hline(aes(yintercept = 8), subset(gammaData,samplingSize==8))+ geom_hline(aes(yintercept = 16), subset(gammaData,samplingSize==16))+ geom_hline(aes(yintercept = 32), subset(gammaData,samplingSize==32))+ geom_hline(aes(yintercept = 64), subset(gammaData,samplingSize==64))+ geom_hline(aes(yintercept = 128), subset(gammaData,samplingSize==128))+theme(panel.grid.major = element_blank())

# to put different ablines in different facets we need to make a dataset, and this would be efficient but the order is all messed up so I had to figure out a not so efficient but working way ;)
#hline.data <- data.frame(z = c(2,4,8,16,32,64,128),samplingSizeF = c(2,4,8,16,32,64,128))
#fig3 + geom_hline(aes(yintercept = z), hline.data)
                                                                                                                                                                                  
```

**Figure 3.** Predictions of sampling size using the gamma distribution method for each population (with thetas 2,10 and 20) against the observed number of haplotypes in the sample. The ablines in each panel represent the "real" sampling size of that sample.  

## Sampling theory
Now we have the dataset "dataSamplingTheory_23Jul2015" and we can plot the expected number of haplotypes for each theta (and the variance).

Now we are going to invert the plots so we have the observed number of haplotypes on the x-axis, and we are going to plot the simulated data with sampling theory, plus the standard error to mark the confidence intervals. On top of that, we add the points of the "observed" data.

```{r sampling theory,message=FALSE, echo=FALSE}
library(ggplot2)

# -----observed data------
# load data of number of haplotypes
data <- read.csv("numberHaplotypes.csv",header=T)


# taking out data with growth
minidata <- subset(data,data$Growth == "no")
minidata[,-1] -> minidata
droplevels(minidata) -> minidata
row.names(minidata) <- NULL # cleaning row.names
#View(minidata)

# creating and id to merge later
myObMatrix <- as.matrix(minidata[,c("Theta","n.haplotypes")])
minidata$id <- apply(myObMatrix, 1, paste, collapse=" ")

# -----predicted data------
# load data of Ewens sampling theory
theory <- read.csv("dataSamplingTheory_21Ago2015.csv",header=T)

# round the expected number of haplotypes
round(theory$exp.haplotypes,0) -> theory$exp.round.hap

# create id to merge
myPredMatrix <- as.matrix(theory[,c("theta.df","exp.round.hap")])
theory$id <- apply(myPredMatrix, 1, paste, collapse=" ")
#View(theory)

#---- merge predicted with observed data-----
newdata <- merge(theory,minidata,by="id",all.y=T)
newdata[is.na(newdata$n.df),] # check NA's
newdata[complete.cases(newdata),]->newdata1

# calculate difference between Ewens n and the simulated n
newdata1$samplingSize-newdata1$n.df -> newdata1$difEwens

# check weird stuff
View(subset(newdata1,newdata1$Theta=='two'))

ggplot(newdata1, aes(x=samplingSize, y=difEwens,color=Theta)) + theme_bw()+geom_point()+geom_point(position = "jitter")+geom_hline(yintercept=0)+facet_grid(Theta~.) 

# ----ploting n vs haplotypes----

# to plot Ewens data
ewens <- ggplot(newdata1, aes(x=n.df, y=exp.round.hap,color=Theta)) + theme_bw()+geom_point()+geom_point(position = "jitter")+facet_grid(Theta~.) 

# to plot our data
sims <- ggplot(newdata1, aes(x=samplingSize, y=n.haplotypes,color=Theta)) + theme_bw()+geom_point()+geom_point(position = "jitter")+facet_grid(Theta~.)

# plot both together (I need to run the function from file first)
multiplot(ewens,sims)
```

## Wakeley and segregating sites

We need to do two things. First, we need to estimate the most probable observed segregating sites number (the mode) for each combination of theta and sampling size. Second, we need to get the maximum value of n according Wakeley for each segregating site using Wakeley's formula. Finally, we merge by segregating site and then we calculate the differece between Wakeley's predictions and the real sampling size.
Because Wakeley's formula was not working for n>60 and our maximum was 32, 2,4,8,16,32 are the only ones consider in this section.

```{r plotting Wakeley's data}
library(ggplot2)
library(lattice)
library(plyr)
library(modeest)

# we have two types of data:
# "WakeleyData"" has the result of Wakeley formula with all the possible vlues and distributions
# "segsites" has the observed number of seg.sites in the simulations
wakdata<-read.csv("WakeleyData.csv",header=T)
segsites<-read.csv("numberSegSites.csv",header=T)

# making factors for the ggplot2 plots
factor(wakdata$segSites)->wakdata$segsites.f
factor(segsites$samplingSize)->segsites$samplingSize.f

# ------segsites
# subsetting data with growth and cleaning non useful columns
segsites1 <- subset(segsites,segsites$Growth=="no")
droplevels(segsites1) -> segsites1
row.names(segsites1) <- NULL
segsites1$Pop <- NULL
segsites1$Growth <- NULL
# keep only the n<=32
segsites2<-segsites1[which(segsites1$samplingSize<33),]

# estimate the mode (the more frequent value) for each combination of sampling size and theta
# the mode is estimated with the function mvl in the package "modeest"
modes <- ddply(segsites2, c("Theta","samplingSize.f"), summarise, mode=unlist((mlv(n.seg.sites))[1]))
modes$samplingSize <- as.numeric(as.character(allwak$samplingSize.f))
modes$k <- as.character(modes$mode)

# creating and id to merge later
myObMatrix <- as.matrix(modes[,c("Theta","k")])
modes$id <- apply(myObMatrix, 1, paste, collapse=" ")


# -------Wakeley's info 
#----make loop to find the best n's-----
thetas<-c("two","ten","twenty")
a<-c(rep("two",135),rep("ten",135),rep("twenty",135))
b<-c(seq(1,135,1),seq(1,135,1),seq(1,135,1))
goodwak<-as.data.frame(cbind(a,b))
names(goodwak)<-c("theta","seg.sites")

for(i in thetas){
  for(j in 1:nrow(goodwak)){ #Paula this wasn't working should 'mydata' be 'goodwak'??
    goodwak$seg.sites[j]->mysegsite
    mysubset<-subset(miniwak,miniwak$Theta==i & miniwak$segSites==mysegsite)
    max(mysubset$prob)->maxprob
    which(mysubset$prob==maxprob)->best
    mysubset$n[best]->best.n
    best.n->goodwak$best.n[j]
    }
}
# now "goodwak" has theta,seg.sites and best.n

# creating and id to merge later
myWakMatrix <- as.matrix(goodwak[,c("theta","seg.sites")])
goodwak$id <- apply(myWakMatrix, 1, paste, collapse=" ")

# Now we want to merge the observed segregating sites with the segregating sites used to estimate Wakeley's formula, and that will allow us to plot the "real" n vs the best.n according Wakeley.
allwak <-merge(modes,goodwak,by="id",all.x=T)

allwak$dif <- allwak$samplingSize - allwak$best.n

ggplot(allwak, aes(x=samplingSize, y=dif,color=Theta)) + theme_bw()+geom_point()+geom_point(position = "jitter")+geom_hline(yintercept=0)

```

```{r}
library(ggplot2)

all <- read.csv("hap_segSites.csv",header=)
factor(all$samplingSize)->all$samplingSize.f

ggplot(all, aes(x=n.haplotypes, y=n.seg.sites)) + theme_bw()+geom_point()+geom_point(position = "jitter")+facet_grid(Theta~samplingSize.f) 

```

```{r distribution of seg sites}

# to check the distribution of seg sites in each combination
# with ggplot2
ggplot(segsites1, aes(n.seg.sites)) + theme_bw()+geom_histogram()+facet_wrap(Theta~samplingSize.f) +theme(panel.grid.major = element_blank())
# with lattice
with(segsites1,histogram(~n.seg.sites|Theta*samplingSize.f))
```

